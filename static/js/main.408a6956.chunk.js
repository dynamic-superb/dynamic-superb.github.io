(this["webpackJsonpdynamic-superb"]=this["webpackJsonpdynamic-superb"]||[]).push([[0],{1021:function(e,t,n){},1022:function(e,t,n){"use strict";n.r(t);var i=n(0),a=n.n(i),s=n(30),r=n.n(s),o=(n(552),n(553),n(45)),c=n(59),h=n(1143),l=n(1152),d=n(504),p=n(1180),m=n(159),u=n(1145),b=n(40),g=n(1025),S=n(257),f=n(1);function L(e){const t=Object(S.a)(),{anchorKey:n,...i}=e;return Object(f.jsxs)(a.a.Fragment,{children:[Object(f.jsx)(p.a,{id:n,position:"relative",top:t.spacing(-4),visibility:"hidden"}),Object(f.jsx)(p.a,{...i})]})}function w(e){const t=Object(S.a)();return Object(f.jsx)(L,{margin:t.spacing(4,"auto",8),...e})}function T(e){const t=Object(S.a)();return Object(f.jsx)(L,{margin:t.spacing(4,"auto",6),...e})}var k=n(42),j=n(1144),y=n(1146),v=n(35);const A=Object(h.a)((e=>({})));function M(e){const{link:t,children:n}=e,i=t?v.a:p.a,a=Object(o.h)();Object(b.a)(),A();return Object(f.jsx)(p.a,{component:"span",fontWeight:a.pathname.includes(t)?"bold":"inherit",children:(t||"").includes("http")?Object(f.jsx)("a",{className:"unlink",href:t||"",target:"_blank",children:n}):Object(f.jsx)(i,{className:"unlink",to:t||"",children:n})})}const R=Object(h.a)((e=>({descriptionButton:{display:"inline-block",margin:e.spacing(1)},pseudoOutlinedPrimaryButton:{backgroundColor:"transparent",border:"1px solid ".concat(Object(k.c)(e.palette.primary.main,.5)),borderRadius:e.shape.borderRadius,display:"inline-block"},innerButton:{paddingLeft:e.spacing(2),paddingRight:e.spacing(2)}})));function _(e){const{name:t,link:n}=e,i=R();return Object(f.jsx)(M,{link:n,children:Object(f.jsx)(j.a,{size:"small",variant:"outlined",className:i.descriptionButton,children:t})})}function C(e){return e.charAt(0).toUpperCase()+e.slice(1)}function x(e){return Object(f.jsx)(p.a,{component:"span",fontWeight:"bold",fontStyle:"italic",children:e.children})}var O=n(462),I=n.n(O),E=n(463),B=n.n(E),W=n(464),P=n.n(W),G=n(252),D=n(1147),$=n(253),N=n(254),z=n(255),H=n(140),F=n.n(H);const V={palette:{primary:{main:G.a[100]},text:{primary:D.a[600],secondary:G.a[600]}}},U=F()(V,{palette:{primary:{main:{$set:$.a[400]}}}}),J=F()(V,{palette:{primary:{main:{$set:N.a[400]}}}}),K=F()(V,{palette:{primary:{main:{$set:z.a[400]}}}}),Y=F()(V,{palette:{primary:{main:{$set:D.a[600]}}}}),Q=[{name:"Content",description:"Content description",tasks:[{name:"Language Identification",description:"Language Identification task is aimed to determine the language spoken in a given speech recording.                    Currently, we are using the VoxForge Dataset for this task."},{name:"Speech Command Recognition",description:"Speech Command Recognition aims to identify the spoken command.                    Currently, datasets utilized for this task is the Google Speech Commands V1 datset."},{name:"Speech Detection",description:"Speech Detection aims to identify whether the given audio clip contains real speech or not.                    Currently, datasets utilized for this task is the LJSpeech dataset, LibriSpeech-TestClean and LibriSpeech-TestOther dataset."},{name:"Speech Text Matching",description:"Speech Text Matching aims to determine if the speech and text are matched.                    Currently, datasets utilized for this task is the LJSpeech dataset, LibriSpeech-TestClean and LibriSpeech-TestOther dataset."},{name:"Spoken Term Detection",description:"Spoken Term Detection aims to check for the existence of the given word in the speech.                    Currently, datasets utilized for this task is the LJSpeech dataset, LibriSpeech-TestClean and LibriSpeech-TestOther dataset."}]},{name:"Speaker",description:"Speaker description",tasks:[{name:"Multi-Speaker Detection",description:"MultiSpeaker Detection aims to analyze the speech audio to determine whether there is more than one speaker present in it.                    Currently, datasets utilized for this task are the LibriSpeech-TestClean dataset and VCTK Dataset."},{name:"Speaker Counting",description:"Speaker Counting aims to identify the total number of speaker in speech audio.                    Currently, datasets utilized for this task is the LibriTTS-TestClean dataset."},{name:"Speaker Verification",description:"Speaker verification aims to verify whether the two given speech audios are from the same speaker.                    Currently, datasets utilized for this task is the LibriSpeech-TestClean dataset and the VCTK dataset."}]},{name:"Semantics",description:"Semantics description",tasks:[{name:"Dialogue Act Classification",description:"Dialogue act classification aims to identify the primary purpose or function of an utterance within its dialogue context.                    Currently, the DailyTalk Dataset is the main dataset used for this task."},{name:"Dialogue Act Pairing",description:"Dialogue act pairing involves assessing the congruence of dialogue acts\u2014that is, whether a response dialogue act is appropriate given a query dialogue act.                    We use the DailyTalk Dataset in this task."},{name:"Intent Classification",description:"Intent classification aims to identify the actionable item behind a spoken message.                    Currently, we utilize the FluentSpeechCommands Dataset for this task."},{name:"Sarcasm Detection",description:"Sarcasm Detection aims to detect if the sarcasm or the irony present in the speech audio.                    Currently, datasets utilized for this task is the MUStARD dataset."}]},{name:"Degradation",description:"Degradation description",tasks:[{name:"Enhancement Detection",description:"Enhancement detection is a task focused on determining whether a given audio has been created or modified by a speech enhancement model.                    Currently, the project employs the LibriTTS-TestClean dataset for this task."},{name:"Noise Detection",description:"Noise Detection aims to idenetify if the speech audio is clean or mixed with noises.                    Currently, datasets utilized for this task are the LJSpeech dataset and VCTK Dataset , and Musan Dataset provides the noise data."},{name:"Noise SNR Level Prediction",description:"Noise SNR Level Prediction aims to predict the signal-to-noise ratio of the speech audio.                Currently, datasets utilized for this task is the VCTK Dataset, and Musan Dataset provides the noise data."},{name:"Reverberation Detection",description:"Reverberation Detection aims to detect if the speech audio is clean or mixed with room impulse responses (RIRs) and noises, that is to say reverberation noises.                Currently, datasets utilized for this task are the LJSpeech dataset and VCTK dataset , and RIRs Noises dataset provides the reverberation noises."}]},{name:"Paralinguistics",description:"Paralinguistics description",tasks:[{name:"Accent Classification",description:"Accent classification involves the recognition and classification of specific speech accents.                    Currently, the AccentDB Extended Dataset is the only dataset utilized for this task."},{name:"Dialogue Emotion Classification",description:"Dialogue emotion classification is a task that assesses an AI model's ability to identify the most suitable emotion in a given dialogue extract.                    In this context, we use the DailyTalk Dataset as our primary data source."},{name:"Emotion Recognition",description:"Emotion recognition aims to identify the most appropriate emotional category for a given utterance.                    Currently, the Multimodal EmotionLines Dataset is the only dataset utilized for this task."},{name:"How Far Are You",description:"The HowFarAreYou task aims to determine the distance of the speaker from the source of sound. It primarily involves using the 3DSpeaker Dataset."},{name:"Spoof Detection",description:"Spoof Detection aims to classify whether the given utterance is a spoofed voice or an authentic recording.                    Currently, datasets utilized for this task is the ASVspoof 2015 dataset and ASVspoof 2017 dataset."},{name:"Stress Detection",description:"Stress Detection aims to determine the stress placement in English vocabulary. Currently, datasets utilized for this task is the MIR-SD dataset."}]},{name:"Audio",description:"Audio description",tasks:[{name:"Accent Classification",description:"Bird sound detection is an interesting task that aims to identify the presence or absence of bird sounds in a given speech.                    The main dataset used for this task is the Warblrb10k Dataset"},{name:"Chord Classification",description:"Chord classification is a crucial task for understanding and analyzing music.                    This endeavor focuses on determining whether a given tune for piano or guitar is in major or minor key.                    Currently, the Acoustic Guitar and Piano Dataset is being used for this task."},{name:"Environmental Sound Classification",description:"Environmental Sound Classification (ESC) focuses on recognizing and classifying sounds that originate from the environment.                    At present, the ESC50 Dataset is specifically used for accomplishing this task in the Dynamic-SUPERB benchmark."}]}],q=(I.a,Object(d.a)(U),B.a,Object(d.a)(J),P.a,Object(d.a)(K),Object(d.a)(Y),Object(d.a)(U),Object(d.a)(J),Object(d.a)(K),Object(d.a)(Y),Object(d.a)(Y),{PR_per_public:{header:"PR public",width:110,higherBetter:!1,isScore:!0,type:"number"},KS_acc_public:{header:"KS public",width:110,higherBetter:!0,isScore:!0,type:"number"},IC_acc_public:{header:"IC public",width:110,higherBetter:!0,isScore:!0,type:"number"},SID_acc_public:{header:"SID public",width:110,higherBetter:!0,isScore:!0,type:"number"},ER_acc_public:{header:"ER public",width:110,higherBetter:!0,isScore:!0,type:"number"},ERfold1_acc_public:{header:"ER fold1",width:110,higherBetter:!0,isScore:!0,type:"number"},ERfold2_acc_public:{header:"ER fold2",width:110,higherBetter:!0,isScore:!0,type:"number"},ERfold3_acc_public:{header:"ER fold3",width:110,higherBetter:!0,isScore:!0,type:"number"},ERfold4_acc_public:{header:"ER fold4",width:110,higherBetter:!0,isScore:!0,type:"number"},ERfold5_acc_public:{header:"ER fold5",width:110,higherBetter:!0,isScore:!0,type:"number"},ASR_wer_public:{header:"ASR public",width:120,higherBetter:!1,isScore:!0,type:"number"},ASR_LM_wer_public:{header:"ASR-LM public",width:140,higherBetter:!1,isScore:!0,type:"number"},QbE_mtwv_public:{header:"QbE public",width:120,higherBetter:!0,isScore:!0,type:"number"},SF_f1_public:{header:"SF-F1 public",width:130,higherBetter:!0,isScore:!0,type:"number"},SF_cer_public:{header:"SF-CER public",width:140,higherBetter:!1,isScore:!0,type:"number"},SV_eer_public:{header:"SV public",width:110,higherBetter:!1,isScore:!0,type:"number"},SD_der_public:{header:"SD public",width:110,higherBetter:!1,isScore:!0,type:"number"},ST_bleu_public:{header:"ST public",width:110,higherBetter:!0,isScore:!0,type:"number"},SE_pesq_public:{header:"SE-PESQ public",width:140,higherBetter:!0,isScore:!0,type:"number"},SE_stoi_public:{header:"SE-STOI public",width:140,higherBetter:!0,isScore:!0,type:"number"},SS_sisdri_public:{header:"SS public",width:110,higherBetter:!0,isScore:!0,type:"number"}}),X={PR_per_hidden_dev:{header:"PR hidden dev",width:140,higherBetter:!1,isScore:!0,type:"number"},SID_acc_hidden_dev:{header:"SID hidden dev",width:140,higherBetter:!0,isScore:!0,type:"number"},ER_acc_hidden_dev:{header:"ER hidden dev",width:140,higherBetter:!0,isScore:!0,type:"number"},ASR_wer_hidden_dev:{header:"ASR hidden dev",width:140,higherBetter:!1,isScore:!0,type:"number"},QbE_map_hidden_dev:{header:"QbE-MAP hidden dev",width:180,higherBetter:!0,isScore:!0,type:"number"},QbE_eer_hidden_dev:{header:"QbE-EER hidden dev",width:180,higherBetter:!1,isScore:!0,type:"number"},SV_eer_hidden_dev:{header:"SV hidden dev",width:140,higherBetter:!1,isScore:!0,type:"number"},SD_der_hidden_dev:{header:"SD hidden dev",width:140,higherBetter:!1,isScore:!0,type:"number"},ST_bleu_hidden_dev:{header:"ST hidden dev",width:140,higherBetter:!0,isScore:!0,type:"number"},SS_sisdri_hidden_dev:{header:"SS hidden dev",width:140,higherBetter:!0,isScore:!0,type:"number"},SE_stoi_hidden_dev:{header:"SE-STOI hidden dev",width:180,higherBetter:!0,isScore:!0,type:"number"},SE_pesq_hidden_dev:{header:"SE-PESQ hidden dev",width:180,higherBetter:!0,isScore:!0,type:"number"}};[.0058,.0736].map((e=>100*e)),[.8100076941,.1632352551].map((e=>100*(1-e))),[.4958333373,.7983333468].map((e=>100*e)),[.4712328911,.6794520617].map((e=>100*e)),[.7356,.2149418249].map((e=>100*(1-e))),[.255671,.127294].map((e=>100*(1-e))),[.157551825,.1048149392].map((e=>100*(1-e))),[.1860194802,.5108585358].map((e=>100*e)),[.3694903255,.1780432165].map((e=>100*(1-e))),[.8433188677,.8520344653].map((e=>100*e));n(560);function Z(e){const t=Object(S.a)(),{title:n="Title",titleColor:i="textPrimary",titleVariant:a="h4",titleComponent:s="h4",description:r=null,descriptionColor:o="textSecondary",descriptionVariant:c="body1",textMaxWidth:h=750,divider:l=!0,titleAlign:d="center",id:u=null}=e;return Object(f.jsx)("div",{id:u,children:Object(f.jsxs)(p.a,{margin:t.spacing(3,"auto"),children:[Object(f.jsxs)(p.a,{margin:t.spacing(2,"center"===d?"auto":0),maxWidth:h,textAlign:d,children:[Object(f.jsx)(m.a,{component:s,color:i,variant:a,children:n}),r&&Object(f.jsx)(p.a,{margin:t.spacing(1,"auto"),children:Object(f.jsx)(m.a,{color:o,variant:c,children:r})})]}),l&&Object(f.jsx)(y.a,{})]})})}function ee(e){return Object(f.jsx)(Z,{...e})}const te=e=>{const[t,n]=Object(i.useState)("");return Object(i.useEffect)((()=>{fetch(e).then((e=>e.text())).then((e=>n(e)))}),[]),t};var ne=n(1178),ie=n(251),ae=n(249),se=n(250),re=n(1182),oe=n(1151);const ce={"task_list.md":"/tasks#top","task_submission.md":"/task-submission","review_process.md":"/review-process","call_for_tasks.md":"/call-for-task"};function he(e){Object(b.a)();const t=e.mdFile,n=e.landing;return Object(f.jsx)(w,{align:"left",children:Object(f.jsx)(ne.a,{children:t,remarkPlugins:[ie.a,se.a],rehypePlugins:[ae.a],components:{code(e){let{node:t,inline:n,className:i,children:a,...s}=e;const r=/language-(\w+)/.exec(i||"");return!n&&r?Object(f.jsx)(re.a,{children:String(a).replace(/\n$/,""),language:r[1],PreTag:"div",style:oe.a,showLineNumbers:!0,...s}):n?Object(f.jsx)("code",{style:{backgroundColor:"#e0e0e0",color:"#000",display:"inline-block",borderRadius:"0.5em",padding:"0 0.25em",fontSize:"90%"},children:a}):Object(f.jsx)(re.a,{children:String(a).replace(/\n$/,""),PreTag:"div",style:oe.a,...s})},a(e){let{href:t,children:n,...i}=e;return ce[t.replace("docs/","")]?Object(f.jsx)(v.a,{to:ce[t.replace("docs/","")],children:n}):/\.md/.exec(t||"")||/^#/.exec(t||"")?Object(f.jsx)(v.a,{to:t.replace("docs/","").replace(".md","").replaceAll("_","-"),children:n}):/http/.exec(t||"")||/@/.exec(t||"")?Object(f.jsx)("a",{href:t,target:"_blank",rel:"noopener noreferrer",...i,children:n}):Object(f.jsx)("a",{href:"https://github.com/dynamic-superb/dynamic-superb/blob/main/"+t,target:"_blank",rel:"noopener noreferrer",...i,children:n})},img(e){let{src:t,...i}=e;return Object(f.jsx)("img",{src:n?t:"docs/".concat(t),alt:t,width:"100%"})},p(e){let{level:t,children:n,...i}=e;n.toString().replace(/ /g,"-");return Object(f.jsx)(m.a,{component:"span",variant:"body1",color:"textSecondary",children:n})},h1(e){let{level:t,children:i,...a}=e;const s=i.toString().replace(/ /g,"-");return n?Object(f.jsx)(f.Fragment,{}):Object(f.jsx)(ee,{id:s,titleComponent:"h1",titleVariant:"h4",titleAlign:"center",title:i})},h2(e){let{level:t,children:i,...a}=e;const s=i.toString().replace(/ /g,"-");return n?Object(f.jsx)(ee,{id:s,titleComponent:"h2",titleVariant:"h5",titleAlign:"left",title:i}):Object(f.jsx)("h2",{id:s,children:i})},h3(e){let{level:t,children:i,...a}=e;const s=i.toString().replace(/ /g,"-");return n?Object(f.jsx)(ee,{id:s,titleComponent:"h3",titleVariant:"h5",titleAlign:"left",title:i}):Object(f.jsx)("h3",{id:s,children:i})},h4(e){let{level:t,children:i,...a}=e;const s=i.toString().replace(/ /g,"-");return n?Object(f.jsx)(ee,{id:s,titleComponent:"h4",titleVariant:"h5",titleAlign:"left",title:i}):Object(f.jsx)("h4",{id:s,children:i})},h5(e){let{level:t,children:i,...a}=e;const s=i.toString().replace(/ /g,"-");return n?Object(f.jsx)(ee,{id:s,titleComponent:"h5",titleVariant:"h5",titleAlign:"left",title:i}):Object(f.jsx)("h5",{id:s,children:i})}}})})}function le(e){const t=Object(b.a)(),n=(Object(i.useRef)(null),te("https://raw.githubusercontent.com/dynamic-superb/dynamic-superb/main/README.md"));return Object(f.jsxs)(p.a,{margin:t.spacing(0,0,8),textAlign:"center",children:[Object(f.jsx)(p.a,{margin:t.spacing(8,"auto",1),children:Object(f.jsx)(m.a,{variant:Object(g.a)(t.breakpoints.up("sm"))?"h2":"h4",color:"textPrimary",children:Object(f.jsx)("strong",{children:"Dynamic-SUPERB"})})}),Object(f.jsx)(p.a,{margin:t.spacing(1,"auto",6),children:Object(f.jsx)(m.a,{variant:Object(g.a)(t.breakpoints.up("sm"))?"h4":"h5",color:"textPrimary",children:"Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech"})}),Object(f.jsx)(he,{mdFile:n,landing:!0}),Object(f.jsx)(T,{children:Object(f.jsx)(u.a,{container:!0,justify:"space-evenly",spacing:0,children:[["ntu-1000.png","https://www.ntu.edu.tw/english/"],["cmu-1000.png","https://www.cmu.edu/"],["ut-austin-1000.png","https://www.utexas.edu/"]].map((e=>Object(f.jsx)(u.a,{item:!0,xs:6,sm:4,md:4,children:Object(f.jsx)("a",{target:"_blank",href:e[1],children:Object(f.jsx)("img",{src:e[0],width:"100%"})})},e[0])))})}),Object(f.jsxs)(p.a,{margin:t.spacing(8,0),textAlign:"center",children:[Object(f.jsx)(ee,{title:""}),Object(f.jsxs)(m.a,{variant:"body1",color:"textSecondary",children:["This website is modified from the source code of"," ",Object(f.jsx)(_,{name:Object(f.jsx)("a",{children:"the SUPERB official website"}),link:"https://github.com/superbbenchmark/superbbenchmark.github.io",target:"_blank",rel:"noopener noreferrer"}),"."]})]})]})}n(779);new Date(2021,8,18),v.a,v.a,new Date(2021,8,18),v.a,Object(h.a)((e=>({taskName:{fontWeight:"bold",marginBottom:e.spacing(2)}})));var de=n(1026);function pe(e){const{elevation:t=3,liftDegree:n=6,...i}=e,[s,r]=a.a.useState(!1);return Object(f.jsx)(de.a,{...i,elevation:s?t+n:t,onMouseOver:()=>{r((e=>!e))},onMouseOut:()=>{r((e=>!e))}})}const me=Object(h.a)((e=>({taskName:{fontWeight:"bold",marginBottom:e.spacing(2)}})));var ue=function(e){const t=me(),n=Object(b.a)(),[i,s]=a.a.useState({top:!1,left:!1,bottom:!1,right:!1});return Object(f.jsxs)(a.a.Fragment,{children:[Object(f.jsxs)(w,{margin:n.spacing(8,"auto",1),children:[Object(f.jsx)(ee,{title:"Tasks",description:Object(f.jsxs)("span",{children:["This list provides an overview of the tasks currently included in the benchmark, but we are actively seeking contributions from the community to expand the range of tasks and make the benchmark more diverse and comprehensive. You can view the performance of each model on the ",Object(f.jsx)(v.a,{to:"/leaderboard",children:"leaderboard"})]})}),Object(f.jsx)(u.a,{container:!0,direction:"row",spacing:2,justify:"center",children:Q.map((e=>{let{name:t}=e;return Object(f.jsx)(u.a,{item:!0,children:Object(f.jsx)(M,{link:"/tasks#".concat(t),children:Object(f.jsx)(j.a,{variant:"outlined",children:C(t.toLowerCase())})})})}))})]}),Q.map((e=>{let{name:i,description:a,tasks:s}=e;return Object(f.jsxs)(w,{anchorKey:i,margin:n.spacing(8,"auto",1),children:[Object(f.jsx)(ee,{title:i}),Object(f.jsx)(u.a,{container:!0,spacing:5,justify:"center",children:s.map((e=>{let{name:i,description:a}=e;return Object(f.jsx)(u.a,{item:!0,xs:12,sm:6,md:4,className:t.taskCard,children:Object(f.jsx)(pe,{elevation:3,children:Object(f.jsxs)(p.a,{padding:n.spacing(3,2,0),children:[Object(f.jsx)(m.a,{color:"textPrimary",variant:"h6",className:t.taskName,children:"".concat(i)}),Object(f.jsx)(m.a,{color:"textSecondary",variant:"body2",children:a}),Object(f.jsx)(p.a,{padding:n.spacing(2,0),children:Object(f.jsx)(m.a,{color:"textSecondary",variant:"body1",align:"right",children:Object(f.jsx)(v.a,{to:"/task/".concat(i.replaceAll(" ","")),children:"details"})})})]})})})}))})]})}))]})};n(1160),n(474),n(1183),n(1189),n(1153);n(125),n(66),n(60),n(1156),n(1028);const be=Object(i.createContext)({isLoggedIn:!1,userId:null,token:null,isAdmin:!1,email:null,login:()=>{},logout:()=>{}});n(475);n(1190),n(1159);Object(h.a)((e=>({paper:{border:"1px solid",padding:e.spacing(.7),backgroundColor:e.palette.background.paper,fontSize:"medium"}})));Object(h.a)((e=>({root:{width:"60%",marginLeft:"auto",marginRight:"auto"},textField:{marginBottom:e.spacing(1),width:"80%"},Button:{display:"block",marginLeft:"auto",marginRight:"auto",marginTop:e.spacing(2),marginBottom:"3%"}})));Object(h.a)((e=>({root:{width:"60%",marginLeft:"auto",marginRight:"auto"},textField:{marginBottom:e.spacing(1),width:"80%"},Button:{display:"block",marginLeft:"auto",marginRight:"auto",marginTop:e.spacing(2),marginBottom:"3%"}})));Object(h.a)((e=>({})));n(1161),n(1179),n(1162),n(487),n(488),n(485),n(184);var ge=n(1177);const Se={"BERT-GSLM":{SpeechDetection:{LJSpeech:99.66,"LibriSpeech-TestClean":97.48,"LibriSpeech-TestOther":77.57},SpeechTextMatching:{LJSpeech:54.11,"LibriSpeech-TestClean":57.82,"LibriSpeech-TestOther":54.02},SpokenTermDetection:{LJSpeech:52.6,"LibriSpeech-TestClean":51.45,"LibriSpeech-TestOther":52.36},EnhancementDetection:{"LibriTTS-TestClean_WHAM":74.26},NoiseDetection:{"LJSpeech_MUSAN-Gaussain":83.17,"VCTK-MUSAN-Gaussain":90.98},NoiseSNRLevelPrediction:{"VCTK_MUSAN-Gaussian":46.41},ReverberationDetection:{"LJSpeech_RirsNoises-SmallRoom":58.61,"VCTK_RirsNoises-SmallRoom":55.92},DialogueEmotionClassification:{DailyTalk:79.7},EmotionRecognition:{MultimodalEmotionlinesDataset:0},SpoofDetection:{ASVspoof2015:88.65,ASVspoof2017:42.57},DialogueActClassification:{DailyTalk:45.65},DialogueActPairing:{DailyTalk:48.6},SpeakerCounting:{"LibriTTS-TestClean":55.45},SpeakerVerification:{"LibriSpeech-TestClean":37.25,VCTK:42.36}},Whisper:{},"ImageBind-LLM":{},"Whisper-LLM":{},"ASR-ChatGPT":{}},fe={instances:[{Instance:"BirdSoundDetection_Warblrb10k","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""},{Instance:"ChordClassification_AcousticGuitarAndPiano","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""},{Instance:"EnvironmentalSoundClassification_ESC50-Animals","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""},{Instance:"EnvironmentalSoundClassification_ESC50-ExteriorAndUrbanNoises","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""},{Instance:"EnvironmentalSoundClassification_ESC50-HumanAndNonSpeechSounds","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""},{Instance:"EnvironmentalSoundClassification_ESC50-InteriorAndDomesticSounds","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""},{Instance:"EnvironmentalSoundClassification_ESC50-NaturalSoundscapesAndWaterSounds","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""},{Instance:"SpeechDetection_LJSpeech","BERT-GSLM":"99.66%",Whisper:"99.99%","ImageBind-LLM":"99.75%","Whisper-LLM":"99.99%","ASR-ChatGPT":""},{Instance:"SpeechDetection_LibriSpeech-TestClean","BERT-GSLM":"97.48%",Whisper:"99.92%","ImageBind-LLM":"89.73%","Whisper-LLM":"99.89%","ASR-ChatGPT":""},{Instance:"SpeechDetection_LibriSpeech-TestOther","BERT-GSLM":"77.57%",Whisper:"99.73%","ImageBind-LLM":"82.85%","Whisper-LLM":"99.66%","ASR-ChatGPT":""},{Instance:"SpeechTextMatching_LJSpeech","BERT-GSLM":"54.11%",Whisper:"96.56%","ImageBind-LLM":"51.40%","Whisper-LLM":"80.68%","ASR-ChatGPT":""},{Instance:"SpeechTextMatching_LibriSpeech-TestClean","BERT-GSLM":"57.82%",Whisper:"91.56%","ImageBind-LLM":"51.53%","Whisper-LLM":"84.77%","ASR-ChatGPT":""},{Instance:"SpeechTextMatching_LibriSpeech-TestOther","BERT-GSLM":"54.02%",Whisper:"91.19%","ImageBind-LLM":"49.74%","Whisper-LLM":"83.67%","ASR-ChatGPT":""},{Instance:"SpokenTermDetection_LJSpeech","BERT-GSLM":"52.60%",Whisper:"96.32%","ImageBind-LLM":"59.77%","Whisper-LLM":"58.97%","ASR-ChatGPT":""},{Instance:"SpokenTermDetection_LibriSpeech-TestClean","BERT-GSLM":"51.45%",Whisper:"92.44%","ImageBind-LLM":"46.07%","Whisper-LLM":"45.34%","ASR-ChatGPT":""},{Instance:"SpokenTermDetection_LibriSpeech-TestOther","BERT-GSLM":"52.36%",Whisper:"90.03%","ImageBind-LLM":"47.53%","Whisper-LLM":"45.02%","ASR-ChatGPT":""},{Instance:"SpeechCommandRecognition_GoogleSpeechCommandsV1","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""},{Instance:"LanguageIdentification_VoxForge","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""},{Instance:"EnhancementDetection_LibriTTS-TestClean_WHAM","BERT-GSLM":"74.26%",Whisper:"63.30%","ImageBind-LLM":"77.07%","Whisper-LLM":"91.52%","ASR-ChatGPT":""},{Instance:"NoiseDetection_LJSpeech_MUSAN-Gaussian","BERT-GSLM":"83.17%",Whisper:"84.79%","ImageBind-LLM":"94.03%","Whisper-LLM":"99.66%","ASR-ChatGPT":""},{Instance:"NoiseDetection_VCTK-MUSAN-Gaussian","BERT-GSLM":"90.98%",Whisper:"78.12%","ImageBind-LLM":"95.85%","Whisper-LLM":"98.11%","ASR-ChatGPT":""},{Instance:"NoiseDetection_LJSpeech_MUSAN-Music","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""},{Instance:"NoiseDetection_VCTK_MUSAN-Music","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""},{Instance:"NoiseDetection_LJSpeech_MUSAN-Noise","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""},{Instance:"NoiseDetection_VCTK_MUSAN-Noise","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""},{Instance:"NoiseDetection_LJSpeech_MUSAN-Speech","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""},{Instance:"NoiseDetection_VCTK_MUSAN-Speech","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""},{Instance:"NoiseSNRLevelPrediction_VCTK_MUSAN-Gaussian","BERT-GSLM":"46.41%",Whisper:"35.21%","ImageBind-LLM":"47.66%","Whisper-LLM":"78.95%","ASR-ChatGPT":""},{Instance:"NoiseSNRLevelPrediction_VCTK_MUSAN-Music","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""},{Instance:"NoiseSNRLevelPrediction_VCTK_MUSAN-Noise","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""},{Instance:"NoiseSNRLevelPrediction_VCTK_MUSAN-Speech","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""},{Instance:"ReverberationDetection_LJSpeech_RirsNoises-LargeRoom","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""},{Instance:"ReverberationDetection_VCTK_RirsNoises-LargeRoom","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""},{Instance:"ReverberationDetection_LJSpeech_RirsNoises-MediumRoom","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""},{Instance:"ReverberationDetection_VCTK_RirsNoises-MediumRoom","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""},{Instance:"ReverberationDetection_LJSpeech_RirsNoises-SmallRoom","BERT-GSLM":"58.61%",Whisper:"86.91%","ImageBind-LLM":"71.79%","Whisper-LLM":"89.39%","ASR-ChatGPT":""},{Instance:"ReverberationDetection_VCTK_RirsNoises-SmallRoom","BERT-GSLM":"55.92%",Whisper:"78.49%","ImageBind-LLM":"85.47%","Whisper-LLM":"88.44%","ASR-ChatGPT":""},{Instance:"AccentClassification_AccentdbExtended","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""},{Instance:"DialogueEmotionClassification_DailyTalk","BERT-GSLM":"79.70%",Whisper:"82.45%","ImageBind-LLM":"80.37%","Whisper-LLM":"82.68%","ASR-ChatGPT":""},{Instance:"EmotionRecognition_MultimodalEmotionlinesDataset","BERT-GSLM":"0.00%",Whisper:"0.03%","ImageBind-LLM":"12.13%","Whisper-LLM":"11.21%","ASR-ChatGPT":""},{Instance:"HowFarAreYou_3DSpeaker","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""},{Instance:"StressDetection_MIRSD","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""},{Instance:"SpoofDetection_ASVspoof2015","BERT-GSLM":"88.65%",Whisper:"94.07%","ImageBind-LLM":"94.19%","Whisper-LLM":"97.76%","ASR-ChatGPT":""},{Instance:"SpoofDetection_ASVspoof2017","BERT-GSLM":"42.57%",Whisper:"21.16%","ImageBind-LLM":"52.53%","Whisper-LLM":"73.64%","ASR-ChatGPT":""},{Instance:"DialogueActClassification_DailyTalk","BERT-GSLM":"45.65%",Whisper:"59.37%","ImageBind-LLM":"44.37%","Whisper-LLM":"61.06%","ASR-ChatGPT":""},{Instance:"IntentClassification_FluentSpeechCommands-Action","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""},{Instance:"IntentClassification_FluentSpeechCommands-Location","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""},{Instance:"IntentClassification_FluentSpeechCommands-Object","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""},{Instance:"SarcasmDetection_Mustard","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""},{Instance:"DialogueActPairing_DailyTalk","BERT-GSLM":"48.60%",Whisper:"50.36%","ImageBind-LLM":"50.88%","Whisper-LLM":"49.84%","ASR-ChatGPT":""},{Instance:"SpeakerCounting_LibriTTS-TestClean","BERT-GSLM":"55.45%",Whisper:"41.65%","ImageBind-LLM":"55.85%","Whisper-LLM":"87.00%","ASR-ChatGPT":""},{Instance:"SpeakerVerification_LibriSpeech-TestClean","BERT-GSLM":"37.25%",Whisper:"74.80%","ImageBind-LLM":"17.80%","Whisper-LLM":"97.99%","ASR-ChatGPT":""},{Instance:"SpeakerVerification_VCTK","BERT-GSLM":"42.36%",Whisper:"50.20%","ImageBind-LLM":"52.68%","Whisper-LLM":"92.00%","ASR-ChatGPT":""},{Instance:"MultiSpeakerDetection_LibriSpeech-TestClean","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""},{Instance:"MultiSpeakerDetection_VCTK","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":""}]},Le={instances:[{Instance:"BirdSoundDetection_Warblrb10k","BERT-GSLM":"0.00%",Whisper:"0.00%","ImageBind-LLM":"28.29%","Whisper-LLM":"14.67%","ASR-ChatGPT":"14.71%"},{Instance:"ChordClassification_AcousticGuitarAndPiano","BERT-GSLM":"0.00%",Whisper:"0.00%","ImageBind-LLM":"44.35%","Whisper-LLM":"58.44%","ASR-ChatGPT":"2.79%"},{Instance:"EnvironmentalSoundClassification_ESC50-Animals","BERT-GSLM":"0.00%",Whisper:"4.00%","ImageBind-LLM":"73.75%","Whisper-LLM":"11.75%","ASR-ChatGPT":"15.50%"},{Instance:"EnvironmentalSoundClassification_ESC50-ExteriorAndUrbanNoises","BERT-GSLM":"0.00%",Whisper:"0.00%","ImageBind-LLM":"48.75%","Whisper-LLM":"3.50%","ASR-ChatGPT":"7.00%"},{Instance:"EnvironmentalSoundClassification_ESC50-HumanAndNonSpeechSounds","BERT-GSLM":"0.00%",Whisper:"1.75%","ImageBind-LLM":"12.00%","Whisper-LLM":"6.00%","ASR-ChatGPT":"19.50%"},{Instance:"EnvironmentalSoundClassification_ESC50-InteriorAndDomesticSounds","BERT-GSLM":"0.00%",Whisper:"0.00%","ImageBind-LLM":"20.25%","Whisper-LLM":"7.75%","ASR-ChatGPT":"4.00%"},{Instance:"EnvironmentalSoundClassification_ESC50-NaturalSoundscapesAndWaterSounds","BERT-GSLM":"0.00%",Whisper:"0.00%","ImageBind-LLM":"22.75%","Whisper-LLM":"9.25%","ASR-ChatGPT":"4.75%"},{Instance:"SpeechDetection_LJSpeech","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":"54.26%"},{Instance:"SpeechDetection_LibriSpeech-TestClean","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":"55.69%"},{Instance:"SpeechDetection_LibriSpeech-TestOther","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":"52.26%"},{Instance:"SpeechTextMatching_LJSpeech","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":"80.99%"},{Instance:"SpeechTextMatching_LibriSpeech-TestClean","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":"52.18%"},{Instance:"SpeechTextMatching_LibriSpeech-TestOther","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":"75.40%"},{Instance:"SpokenTermDetection_LJSpeech","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":"48.04%"},{Instance:"SpokenTermDetection_LibriSpeech-TestClean","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":"79.58%"},{Instance:"SpokenTermDetection_LibriSpeech-TestOther","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":"78.73%"},{Instance:"SpeechCommandRecognition_GoogleSpeechCommandsV1","BERT-GSLM":"0.08%",Whisper:"8.46%","ImageBind-LLM":"4.31%","Whisper-LLM":"3.90%","ASR-ChatGPT":"41.61%"},{Instance:"LanguageIdentification_VoxForge","BERT-GSLM":"0.00%",Whisper:"20.28%","ImageBind-LLM":"26.99%","Whisper-LLM":"13.54%","ASR-ChatGPT":"96.53%"},{Instance:"EnhancementDetection_LibriTTS-TestClean_WHAM","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":"63.32%"},{Instance:"NoiseDetection_LJSpeech_MUSAN-Gaussian","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":"49.02%"},{Instance:"NoiseDetection_VCTK-MUSAN-Gaussian","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":"50.28%"},{Instance:"NoiseDetection_LJSpeech_MUSAN-Music","BERT-GSLM":"60.53%",Whisper:"49.46%","ImageBind-LLM":"51.92%","Whisper-LLM":"60.49%","ASR-ChatGPT":"49.62%"},{Instance:"NoiseDetection_VCTK_MUSAN-Music","BERT-GSLM":"25.78%",Whisper:"49.80%","ImageBind-LLM":"53.34%","Whisper-LLM":"57.99%","ASR-ChatGPT":"49.30%"},{Instance:"NoiseDetection_LJSpeech_MUSAN-Noise","BERT-GSLM":"48.84%",Whisper:"50.58%","ImageBind-LLM":"55.29%","Whisper-LLM":"59.01%","ASR-ChatGPT":"49.52%"},{Instance:"NoiseDetection_VCTK_MUSAN-Noise","BERT-GSLM":"9.23%",Whisper:"51.18%","ImageBind-LLM":"57.08%","Whisper-LLM":"58.26%","ASR-ChatGPT":"49.61%"},{Instance:"NoiseDetection_LJSpeech_MUSAN-Speech","BERT-GSLM":"53.20%",Whisper:"49.76%","ImageBind-LLM":"50.21%","Whisper-LLM":"66.68%","ASR-ChatGPT":"49.63%"},{Instance:"NoiseDetection_VCTK_MUSAN-Speech","BERT-GSLM":"19.00%",Whisper:"51.45%","ImageBind-LLM":"48.32%","Whisper-LLM":"61.91%","ASR-ChatGPT":"49.96%"},{Instance:"NoiseSNRLevelPrediction_VCTK_MUSAN-Gaussian","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":"19.82%"},{Instance:"NoiseSNRLevelPrediction_VCTK_MUSAN-Music","BERT-GSLM":"28.99%",Whisper:"20.73%","ImageBind-LLM":"17.79%","Whisper-LLM":"20.47%","ASR-ChatGPT":"19.41%"},{Instance:"NoiseSNRLevelPrediction_VCTK_MUSAN-Noise","BERT-GSLM":"26.83%",Whisper:"22.06%","ImageBind-LLM":"21.98%","Whisper-LLM":"22.70%","ASR-ChatGPT":"19.30%"},{Instance:"NoiseSNRLevelPrediction_VCTK_MUSAN-Speech","BERT-GSLM":"19.63%",Whisper:"26.39%","ImageBind-LLM":"18.25%","Whisper-LLM":"20.17%","ASR-ChatGPT":"19.18%"},{Instance:"ReverberationDetection_LJSpeech_RirsNoises-LargeRoom","BERT-GSLM":"62.56%",Whisper:"89.39%","ImageBind-LLM":"43.14%","Whisper-LLM":"89.75%","ASR-ChatGPT":"48.16%"},{Instance:"ReverberationDetection_VCTK_RirsNoises-LargeRoom","BERT-GSLM":"60.88%",Whisper:"85.93%","ImageBind-LLM":"73.14%","Whisper-LLM":"78.80%","ASR-ChatGPT":"47.47%"},{Instance:"ReverberationDetection_LJSpeech_RirsNoises-MediumRoom","BERT-GSLM":"67.12%",Whisper:"89.12%","ImageBind-LLM":"51.75%","Whisper-LLM":"90.06%","ASR-ChatGPT":"47.87%"},{Instance:"ReverberationDetection_VCTK_RirsNoises-MediumRoom","BERT-GSLM":"57.74%",Whisper:"84.00%","ImageBind-LLM":"77.17%","Whisper-LLM":"80.04%","ASR-ChatGPT":"47.76%"},{Instance:"ReverberationDetection_LJSpeech_RirsNoises-SmallRoom","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":"48.26%"},{Instance:"ReverberationDetection_VCTK_RirsNoises-SmallRoom","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":"48.00%"},{Instance:"AccentClassification_AccentdbExtended","BERT-GSLM":"0.00%",Whisper:"0.00%","ImageBind-LLM":"19.08%","Whisper-LLM":"4.33%","ASR-ChatGPT":"6.63%"},{Instance:"DialogueEmotionClassification_DailyTalk","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":"57.76%"},{Instance:"EmotionRecognition_MultimodalEmotionlinesDataset","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":"50.03%"},{Instance:"HowFarAreYou_3DSpeaker","BERT-GSLM":"0.00%",Whisper:"0.00%","ImageBind-LLM":"7.42%","Whisper-LLM":"0.05%","ASR-ChatGPT":"0.50%"},{Instance:"StressDetection_MIRSD","BERT-GSLM":"37.89%",Whisper:"25.47%","ImageBind-LLM":"35.20%","Whisper-LLM":"15.52%","ASR-ChatGPT":"15.32%"},{Instance:"SpoofDetection_ASVspoof2015","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":"14.75%"},{Instance:"SpoofDetection_ASVspoof2017","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":"15.34%"},{Instance:"DialogueActClassification_DailyTalk","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":"62.88%"},{Instance:"IntentClassification_FluentSpeechCommands-Action","BERT-GSLM":"0.00%",Whisper:"1.28%","ImageBind-LLM":"20.53%","Whisper-LLM":"17.29%","ASR-ChatGPT":"85.67%"},{Instance:"IntentClassification_FluentSpeechCommands-Location","BERT-GSLM":"0.00%",Whisper:"1.82%","ImageBind-LLM":"14.30%","Whisper-LLM":"14.94%","ASR-ChatGPT":"83.85%"},{Instance:"IntentClassification_FluentSpeechCommands-Object","BERT-GSLM":"0.00%",Whisper:"0.22%","ImageBind-LLM":"13.31%","Whisper-LLM":"4.83%","ASR-ChatGPT":"75.89%"},{Instance:"SarcasmDetection_Mustard","BERT-GSLM":"21.16%",Whisper:"51.74%","ImageBind-LLM":"50.72%","Whisper-LLM":"46.52%","ASR-ChatGPT":"64.06%"},{Instance:"DialogueActPairing_DailyTalk","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":"47.45%"},{Instance:"SpeakerCounting_LibriTTS-TestClean","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":"19.60%"},{Instance:"SpeakerVerification_LibriSpeech-TestClean","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":"45.44%"},{Instance:"SpeakerVerification_VCTK","BERT-GSLM":"",Whisper:"","ImageBind-LLM":"","Whisper-LLM":"","ASR-ChatGPT":"42.62%"},{Instance:"MultiSpeakerDetection_LibriSpeech-TestClean","BERT-GSLM":"22.60%",Whisper:"56.40%","ImageBind-LLM":"37.55%","Whisper-LLM":"59.25%","ASR-ChatGPT":"46.80%"},{Instance:"MultiSpeakerDetection_VCTK","BERT-GSLM":"42.95%",Whisper:"59.65%","ImageBind-LLM":"53.25%","Whisper-LLM":"62.00%","ASR-ChatGPT":"47.25%"}]},we=["Model Name","BirdSoundDetection","ChordClassification","EnvironmentalSoundClassification","SpeechDetection","SpeechTextMatching","SpokenTermDetection","SpeechCommandRecognition","LanguageIdentification","EnhancementDetection","NoiseDetection","NoiseSNRLevelPrediction","ReverberationDetection","AccentClassification","DialogueEmotionClassification","EmotionRecognition","HowFarAreYou","StressDetection","SpoofDetection","DialogueActClassification","IntentClassification","SarcasmDetection","DialogueActPairing","SpeakerCounting","SpeakerVerification","MultiSpeakerDetection"],Te=["BERT-GSLM","Whisper","ImageBind-LLM","Whisper-LLM","ASR-ChatGPT"],ke=[];for(const At of we){const e={field:At,width:150,headerAlign:"center",align:"center",headerClassName:"data-grid-header"};ke.push(e)}const je=[];let ye=1;for(const At in Se){const e={id:ye,"Model Name":At};for(const t in Se[At]){const n=Object.keys(Se[At][t]);let i=0;for(const e of n)i+=Se[At][t][e];let a=0;a=Math.round(i/n.length*100)/100,e[t]=a}je.push(e),ye++}const ve=[];ve.push({field:"Task",width:200,headerAlign:"left",align:"left",headerClassName:"data-grid-header"}),ve.push({field:"Dataset",width:200,headerAlign:"left",align:"left",headerClassName:"data-grid-header"});for(const At of Te){const e={field:At,width:150,headerAlign:"center",align:"center",headerClassName:"data-grid-header",renderCell:e=>"N/A"!==e.value?e.row.best===At?Object(f.jsx)("strong",{children:e.value}):Object(f.jsx)("span",{children:e.value}):Object(f.jsx)("span",{style:{color:"#d0d0d0"},children:"N/A"})};ve.push(e)}const Ae=e=>"N/A"!==e?parseFloat(e.slice(0,-1)):-1,Me=[],Re=[],_e=[];for(let At=0;At<fe.instances.length;At++){const e=fe.instances[At],t=Le.instances[At],n=e.Instance.indexOf("_"),i=e.Instance.slice(0,n),a=e.Instance.slice(n+1),s={id:At,Task:i,Dataset:a},r={id:At,Task:i,Dataset:a},o={id:At,Task:i,Dataset:a};for(const c of Te)s[c]=e[c]||"N/A",r[c]=t[c]||"N/A",o[c]=e[c]||t[c];s.best=Te.reduce((function(e,t){return Ae(s[e])>Ae(s[t])?e:t})),r.best=Te.reduce((function(e,t){return Ae(r[e])>Ae(r[t])?e:t})),o.best=Te.reduce((function(e,t){return Ae(o[e])>Ae(o[t])?e:t})),Me.push(s),Re.push(r),_e.push(o)}var Ce=n(10),xe=n(1027),Oe=n(1192),Ie=n(1181),Ee=n(1164),Be=n(493),We=n.n(Be);const Pe=Object(Ce.a)(ge.a)((e=>{let{theme:t}=e;return{"& .MuiDataGrid-columnHeader, .MuiDataGrid-cell":{borderRight:"1px solid #d0d0d0"},"& .MuiDataGrid-columnHeader":{backgroundColor:"#e0e0e0"}}}));function Ge(){const[e,t]=Object(i.useState)(_e),[n,a]=Object(i.useState)(ve),[s,r]=i.useState(0),[o,c]=i.useState(null),h=Boolean(o),l=h?"simple-popover":void 0;return Object(f.jsxs)("div",{style:{width:"100%",height:"calc(100vh - 160px)",maxWidth:"1170px",margin:"auto"},children:[Object(f.jsxs)(de.a,{variant:"outlined",children:[Object(f.jsxs)(p.a,{sx:{height:"48px",display:"flex",justifyContent:"flex-start",alignItems:"center",gap:"0.5em"},children:[Object(f.jsx)("div",{style:{width:"1em"}}),Object(f.jsx)(m.a,{variant:"h5",children:"Leaderboard"}),Object(f.jsx)(xe.a,{size:"small",onClick:e=>{c(e.currentTarget)},children:Object(f.jsx)(We.a,{fontSize:"inherit"})}),Object(f.jsx)(Oe.a,{id:l,open:h,anchorEl:o,onClose:()=>{c(null)},anchorOrigin:{vertical:"bottom",horizontal:"center"},transformOrigin:{vertical:"top",horizontal:"center"},children:Object(f.jsxs)(m.a,{style:{padding:"1em"},children:[Object(f.jsx)("strong",{children:"Seen"}),": The model has been trained on this task.",Object(f.jsx)("br",{}),Object(f.jsx)("strong",{children:"Unseen"}),": The model has not been trained on this task.",Object(f.jsx)("br",{}),"All metrics for the current benchmark tasks are ",Object(f.jsx)("strong",{children:"ACCURACY"}),"."]})})]}),Object(f.jsx)(p.a,{sx:{backgroundColor:"#f0f0f0"},children:Object(f.jsxs)(Ie.a,{value:s,indicatorColor:"secondary",textColor:"secondary",onChange:(e,n)=>{r(n),0===n?t(_e):1===n?t(Me):2===n&&t(Re)},children:[Object(f.jsx)(Ee.a,{label:"All"}),Object(f.jsx)(Ee.a,{label:"Seen"}),Object(f.jsx)(Ee.a,{label:"Unseen"})]})})]}),Object(f.jsx)(Pe,{rows:e,columns:n,disableRowSelectionOnClick:!0,hideFooter:!0})]})}n(505),n(256);var De,$e=n(49);String.raw(De||(De=Object($e.a)(['\n# SUPERB Challenge\n\n## Evaluation Framework\n\n### Background\n\n![](https://i.imgur.com/FDARwvz.png)\n*Fig 1.*\n\nSUPERB Challenge follows the similar evaluation framework introduced in [SUPERB Benchmark](https://arxiv.org/abs/2105.01051), which benchmarks the **generalizability** of Self-Supervised Learning (SSL) on speech. SSL models are termed **Upstream** and are evaluated with various **Downstream** tasks. The framework extract **multiple frozen hidden states** from a single upstream model and trains a learnable **weighted-sum** over the hidden states along with the downstream model task-by-task.\n\n### Overview\n\n![](https://i.imgur.com/BNr2gfE.png)\n*Fig 2.*\n\nFig 2. illustrates the evaluation framework of the challenge. The challenge evaluates SSL models\' generalizability on 10 tasks. Each of the tasks has a corresponding public dataset (**public-set**) that is publicly available, and a hidden dataset (**hidden-set**) that will not be released. Participants can practice on the public-set to understand the performance of their upstream models, and choose the best one for submission as they wish. Then, participants **submit the upstream model** (model definition & pre-trained weights) publicly or privately to the hidden-set leaderboard. **We finetune the downstream models on the hidden-set** without releasing any audio/label. Both public-set and hidden-set have leaderboards and welcome submissions to share more results with the community. **The winners of the challenge will be solely determined by the ranking on the hidden-set leaderboard.** Finally, there will be **overall metrics** for ranking all upstreams.\n\nAll the participants are encouraged to submit papers to [*AAAI workshop: The 2nd Self-supervised Learning for Audio and Speech Processing*](https://aaai-sas-2022.github.io/). The winners of the challenge will be invited to present their methods in the workshop. We plan to collaborate with more conferences for participants to present their works and papers.\n\n### Tasks\n\n10 evaluation tasks are included in this challenge:\n\n- **Content**\n    - Phoneme Recognition (PR)\n    - Automatic Speech Recognition (ASR)\n    - Query-by-example Spoken Term Detection (QbE)\n- **Speaker**\n    - Speaker Identification (SID)\n    - Automatic Speaker Verification (ASV)\n    - Speaker Diarization (SD)\n- **Paralinguistics**\n    - Emotion Recognition (ER)\n- **Semantics**\n    - Speech Translation (ST)\n- **Generation**\n    - Speech Enhancement (SE)\n    - Speech Separation (SS)\n\nMore task descriptions for the public-set can be found in [TASKS](https://superbbenchmark.org/tasks), and we implement the evaluation scripts for public-set in [S3PRL](https://github.com/s3prl/s3prl/blob/master/s3prl/downstream/docs/superb.md) for reference. The task design and evaluation pipeline will be the same between public-set & hidden-set unless otherwise mentioned.\n\n### Secret tasks\n\nSecret tasks evaluate SSL models\' generalizability on completely unseen tasks. Secret tasks are only present in the hidden-set, and the task design will not be revealed until the final winner announcement. \n\n### What is new\n\nCompared with SUPERB Benchmark, SUPERB Challenge extends the framework with the following:\n\n- **New Tasks**: Speech Translation, Speech Enhancement, Source Separation and secret tasks.\n- **New Data Domains**: A challenging and newly recorded hidden-set with unseen (to upstream) text/audio domain.\n- **New Overall Metrics**: The metrics to rank upstreams.\n\n\n## Upstream Specification\n\n### Unlabeled data only: Focus on SSL\n\n- Any labeled/parallel data made by human annotators are **not allowed** to used for both model training and data preprocessing, e.g.\n    - **audio/text pairs:** transcriptions in English, foreign languages, or phonemes.\n    - **audio/tagging pairs:** speaker labels or sound event labels.\n    - **audio/audio pairs:** audios with the same properties made parallel by human, e.g. audios with same content from different speakers, or the opposite.\n- Any system pre-trained by labeled/parallel data **cannot** be used to help with the SSL pre-training, like pre-trained ASR.\n- Any unlabeled/unparallel data is allowed, including the downstream datasets in the public-set. The nature alignments (not made by human annotators) bettwen audio and other modalities are also allowed, e.g. videos.\n- If it is hard to define whether your data is labeled/parallel, please [contact us](#Contact)!\n\n### Programming Language\n\n- We currently support:\n    - **Python >= 3.6**\n    - **Pytorch >= 1.7**\n\n- We expect the upstream submission can pass the following check:\n    ~~~python=\n    upstream = YourModel.cuda()\n    assert isinstance(upstream, torch.nn.Module)\n    ~~~\n\n\nWe accept upstream models in PyTorch by default. If you wish to submit upstreams in non-PyTorch frameworks, please [fill this form](https://docs.google.com/forms/d/e/1FAIpQLSe52jYL2Yk9oYqXfg_Bg0Sjp01a6HSLUhY5VohsZOE5sNmgsw/viewform)!\nIf you are not feasible to submit the pre-trained model, please [fill this form](https://docs.google.com/forms/d/e/1FAIpQLSdA44nArlIDfGV63WwtwXer4WAPQO1aBwEpAjDSNjbMQN-GJQ/viewform) for us to see how to help!\n\n### Interface functions\n\n#### forward\n\nExtract features from waveforms.\n\n- **Input:** A list of waveforms in 16000 Hz\n\n    ~~~python=\n    SAMPLE_RATE = 16000\n    BATCH_SIZE = 8\n    EXAMPLE_SEC = 10\n    wavs = [torch.randn(SAMPLE_RATE * EXAMPLE_SEC).cuda() for _ in range(BATCH_SIZE)]\n    results = upstream(wavs)\n    ~~~\n\n- **Output:** A dictionary with a key for each task, and a single key for all secret tasks. If any task-specific key is not presented, a "hidden_states" key should be provided as the default key. The value for each key is **a list** of padded sequences in the same shape of **(batch_size, max_sequence_length_of_batch, hidden_size)** for weighted-sum to work. It is welcomed to perform some preprocessing on the upstream\'s raw hidden-sets, including upsampling and downsampling. However, all the values must come from **a single upstream model**:\n\n    ~~~python=\n    assert isinstance(results, dict)\n    tasks = ["PR", "SID", "ER", "ASR", "ASV", "SD", "QbE", "ST", "SS", "SE", "secret"]\n    for task in tasks:\n        hidden_states = results.get(task, "hidden_states")\n        assert isinstance(hidden_states, list)\n\n        for state in hidden_states:\n            assert isinstance(state, torch.Tensor)\n            assert state.dim() == 3, "(batch_size, max_sequence_length_of_batch, hidden_size)"\n            assert state.shape == hidden_states[0].shape\n    ~~~\n\n#### get_downsample_rates\n\nProvide the downsample rate **from 16000 Hz waveforms** for each task\'s representation in the dict. For the standard 10ms stride representation, the downsample rate is 160.\n\n~~~python=\nSAMPLE_RATE = 16000\nMSEC_PER_SEC = 1000\ndownsample_rate = SAMPLE_RATE * 10 / MSEC_PER_SEC  # 160\n~~~\n\nThe downsample rate will be used to:\n\n1. Calculate the valid representation length of each utterance in the output padded representation.\n2. Prepare the training materials according to the representation\'s downsample rate for frame-level tasks: SD, SE, SS.\n\n- **Input:** the task key (str)\n- **Output:** the downsample rate (int) of the representation for that task\n\n~~~python=\nfor task in tasks:\n    assert isinstance(task, str)\n    downsample_rate = upstream.get_downsample_rate(task)\n    assert isinstance(downsample_rate, int)\n    print("The upstream\'s representation for {task}"\n        f" has the downsample rate of {downsample_rate}.")\n~~~\n\n## Public-set and S3PRL toolkit\n\n### As the task definition and demonstration\n\nThe public-set serves as the demonstration of the task design: including the data preprocessing, tasks\' input/output formats and task-specific metrics. The datasets used in the public-set are all chosen to be public available for everyone to participate. Please refer to [TASKS](https://superbbenchmark.org/tasks) and the implementation in [S3PRL](https://github.com/s3prl/s3prl/blob/master/s3prl/downstream/docs/superb.md) for details.\n\n### As the platform for developing upstreams for the hidden-set\n\nThe differences between the public-set and the hidden-set are controlled to be only the following:\n\n1. Recording conditions\n2. Spoken content / text scripts\n3. Speakers\n4. Fewer labeled data\n\nIn this way, the public-set is still a good indicator of the hidden-set performance to some degree. We follow the same (unless mentioned otherwise in [TASKS](/tasks)) implementation in the public-set for the hidden-set, and hence encourage participants to use [S3PRL](https://github.com/s3prl/s3prl/blob/master/s3prl/downstream/docs/superb.md) to benchmark their upstream models (optional) on the public-set. The winners of the challenge will be decided solely on the hidden-set, and the public-set and [S3PRL](https://github.com/s3prl/s3prl/blob/master/s3prl/downstream/docs/superb.md) implementations can serve as the start-kit.\n\n### Provide baselines for comparison\n\n#### Baselines\n\nWe collected most of the well-known SSL baseline models in [S3PRL](https://github.com/s3prl/s3prl/blob/master/s3prl/downstream/docs/superb.md), including TERA, wav2vec2, Hubert, DeCoAR 2.0, and more. You can easily benchmark different upstreams by specifying in the command line arguments.\n\n#### Comparison\n\nSince the full benchmarking on the public-set can take some time for the training to converge. We released the [training artifacts](https://github.com/s3prl/s3prl/blob/master/s3prl/downstream/docs/superb_artifacts.md) of the top baseline systems (e.g. wav2vec2, HuBERT) for participants to quickly compare with them. The artifacts include:\n\n- Tensorboard logs\n- Trained downstream weights (the best on public dev set)\n\n### Public-set leaderboard and submission\n\nThe [public-set leaderboard](leaderboard?subset=Public+Set&track=constrained) is online and [accepts submissions](submit?type=public). There is no deadline. Since all the train/dev/test splits are public available, **the leaderboard accepts submissions with the inferenced prediction files on each task\'s testing split** which will be auto-generated if you follow the benchmarking steps in [S3PRL](https://github.com/s3prl/s3prl/blob/master/s3prl/downstream/docs/superb.md).\n\n## Hidden-set\n\n### Fairness & prevent overfitting\n\nSince all the train/dev/test splits are public in the public-set, it is possible to cheat by directly reporting the best results on the testing split, and the results are thus overfit on the testing split. Hence, the hidden-set is collected and prepared to follow the same task design as that in the public-set but with the newly created data. All the splits will **NOT be released in both audio and labels**. The members involved in the hidden-set preparation should **NOT** participate the challenge. These members are listed in the **Hidden-set Committee** below.\n\n### Hidden-set leaderboard and submission\n\n#### Submission type\n\nThe leaderboard accepts **submissions with the upstream model solely**, including **model definition** and **pre-trained weights**. The upstream model should follow the specification detailed at [Upstream Specification](#Upstream-Specification). The submission can be done publicly or privately. Only the **Hidden-set Committee** members can access the privately submitted upstreams and the models will be used solely for this challenge.\n\n- We accept upstream models in PyTorch by default. If you wish to submit upstreams in non-PyTorch frameworks, please [fill this form](https://docs.google.com/forms/d/e/1FAIpQLSe52jYL2Yk9oYqXfg_Bg0Sjp01a6HSLUhY5VohsZOE5sNmgsw/viewform)!\n- If you are not feasible to submit the pre-trained model, please [fill this form](https://docs.google.com/forms/d/e/1FAIpQLSdA44nArlIDfGV63WwtwXer4WAPQO1aBwEpAjDSNjbMQN-GJQ/viewform) for us to see how to help!\n\n#### Finetuning on submission\n\nAfter the upstream model is submitted, we **benchmark the submitted upstream by finetuning each task\'s downstream model for participants**. **The quota for submissions per week is limited and starts from 2 times/week,** but will be dynamically adjusted based on the number of participants. The quota adjustment will be announced at [NEWS](https://superbbenchmark.org/news). Participants can [contact us](#Contact) to acquire the finetuning artifacts of their own submissions for sanity checks, including:\n\n- Tensorboard logs\n- Testing results\n- Trained downstream weights\n\n#### Practice (dev) / Private (test) scores\n\nAfter training the downstream model for all tasks, we show the best performance on the hidden-set\'s development splits as the **practice scores** (one score per task). The true performance on the testing splits, termed **private scores**, will be revealed along with the final winner announcement. The final team ranking will depend only on the hidden-set\'s **private scores**.\n\n#### How to submit\n\n(Update 10/20/2021 AOE)\n\nThe [hidden-set leaderboard](leaderboard?subset=Hidden+Dev+Set&track=constrained) is online and accepts submissions. Please follow [the submission steps](https://huggingface.co/superb/superb-submission). We use HuggingFace\'s Hub to host the submitted upstreams and track the submitted model weights. On the other hand, we use our [submission page](submit?type=hidden) to control the submission limit, where the participants tell us the locations of their models on HuggingFace\'s Hub.\n\nAfter submitting via the submission page, you will see a new entry in the **submission history** under your Profile page, and the evaluation results on the hidden-set will be revealed at the entry within one week.\nPlease stay tuned!\n\n(Update 10/31/2021 AOE)\n\nAll the submitted results will remain anonymous during and after the challenge.\nYou can select a few submissions to show on the leaderboard.\nIf no submission is selected, we will show all your submissions on the leaderboard anonymously.\nOnly you can see your personal/model information on the leaderboard, but you will not see others\'.\nIf you wish to reveal your personal/model information on the [hidden-set leaderboard](leaderboard?subset=Hidden+Dev+Set&track=constrained), please [contact us](#Contact)!\n\n## Overall Metrics\n\n(Update 10/1/2021 AOE)\n\nWe announce two kinds of metrics: **superb-rank** and **superb-score**, each with parameter-agnostic and parameter-panelized versions.\n\nType|Parameter-agnostic|Parameter-penalized\n-|-|-\nScoring|$superb_s$|$superb_{sp}$\nRanking|$superb_r$|$superb_{rp}$\n\nIn this challenge, $superb_r$ and $superb_{rp}$ are the primary metrics. When equal rank is found on two different upstreams, $superb_{s}$ or $superb_{sp}$ is used to break the tie. Hence, there will be only two final lists of winners: **Parameter-agnostic** and **Parameter-penalized**.\n\n### Notation\n\nSuppose each task $t$ in all tasks $T$ has a single metric $s_t$, and the score of an upstream $u$ on task $t$ is $s_t(u)$ which has already been transformed to make higher values represent better performance. Eg. We use WAcc here for ASR instead of the raw WER. The upstream $u$ has $|u|$ millions parameters.\n\n### Parameter-agnostic\n\nParameter-agnostic metrics demonstrate the best performance SSL can achieve, and encourages participants to explore any possibility to push the limits.\n\n#### $superb_s$\n\nTo aggregate all task-specific scores $s_t(u)$ into a single static score, we linearly transform each of them into points so that:\n\n- $s_t(fbank) = 0$, The performance of FBANK maps to 0\n- $s_t(sota) = 1000$, The performance of the existing SOTA upstream *for this task* maps to 1000.\n\n$$\np_t(u) = dfrac{1000}{s_t({sota}) - s_t({fbank})} ( s_t(u) - s_t({fbank}) )\n$$\n\nHence, most of the points will sit between 0~1000. The upstream worse than FBANK on this task will get negative points. The upstream better than the task-specific SOTA upstream will get points higher than 1000. The $superb_s$ of the upstream $u$ is the average of $p_t$ over all tasks.\n\n$$\nsuperb_s = dfrac{1}{|T|} sum_{t in T} p_t(u)\n$$\n\nIntuitively, two reference points: FBANK and SOTA decide the typical interested interval for a task-specific metric and scale the task scores to 0~1000 points accordingly. The similar range of points across tasks can then be averaged. Beyond scaling with the pre-defined metric range, this interval further determines *how hard for a task to improve*. For a harder task, its smaller interval at the denominator give the task more credit for any unit improvement.\n\n#### $superb_r$\n\nTo encourage the development on **universal models** instead of models skewed toward a subset of tasks. We use ranking to saturate the improvement when an upstream already become the best for that task. The $superb_r$ for an upstream $u$ is the average number of upstreams which $u$ can win in each task. This metric dynamically depends on all the upsreams $U$ shown on the leaderboard. In the following, $L$ is the number of upstreams which $u$ can win using the metric $x_t$.\n\n$$\nL(x_t, u) = | { hat{u} in U | x_t(u) > x_t(hat{u})  } |\n$$\n\n$$\nsuperb_r = dfrac{1}{|T|} sum_{t in T} L(s_t, u) = dfrac{1}{|T|} sum_{t in T} L(p_t, u)\n$$\n\n### Parameter-penalized\n\nTo encourage the development of speech SSL on small and green models, we add a metric with penalization on models\' parameter size. To gauge the effectiveness and parameter-efficiency of new SSL algorithms, we encourage participants to submit multiple upstreams trained by the same algorithm with the only difference in parameter size.\n\n#### $superb_{sp}$\n\nWe penalize the parameter-agnostic scoring by an upstream $u$\'s parameter size $|u|$.\n\n$$\nhat{p_t}(u) =\nleft{\n    \begin{array}{lr}\n        \frac{p_t(u)}{ max(|u|, 1)},& \text{if } p_t(u) geq 0\\\n            p_t(u),              & \text{otherwise}\n    end{array}\n\right.\n$$\n\nSince either the point of the baseline FBANK or its parameter size is 0. The above formula measures how many improvement upon FBANK per parameter. The minimum 1 million parameter size is designed to avoid too small upstreams dominating others by receiving too much credit on a single task. When $p_t(u)$ is negative we do not penalize it, since it is already worse than the zero-parameter FBANK.\n\n$$\nsuperb_{sp} = dfrac{1}{|T|} sum_{t in T} hat{p_t}(u)\n$$\n\n#### $superb_{sp}$\n\nSimilar to $superb_r$, we rank all upstreams on the leaderboard with $hat{p_t}$.\n\n$$\nsuperb_{rp} = dfrac{1}{|T|} sum_{t in T} L(hat{p_t}, u)\n$$\n\n### Conclusion\n\nThe ranking metrics are the primary measures in this challenge and are designed to emphasize an upstream\'s universally usability. When two upstreams tie on the same rank, the scoring metrics take the tasks\' variations and improvement difficulty into account to help the final decision. You can refer to the [public-set leaderboard](leaderboard?subset=Public+Set&track=constrained) for the overal metrics calculation.\n\n#### Reference points\n\nIf a task have multiple metrics, each metric is first tranformed into points or ranks as illustrated above. Then, points or ranks are first averaged in intra-task fashion before being averaged with other task.\n\nTask|PR|SID|ER|ASR|QbE|ASV|SD|ST|SE|SE|SS\n-|-|-|-|-|-|-|-|-|-|-|-\nMetrics|PER|ACC|ACC|WER|MTWV|EER|DER|BLEU|PESQ|STOI|SS\nFBANK|82.01|41.38|48.24|23.18|0.58|9.56|10.05|2.32|2.55|0.9364|9.234\nSOTA|3.53|96.66|67.62|3.62|7.36|5.62|5.11|20.01|2.64|0.9418|10.45\n\n## Winner Minimum Requirements\n\nThe following describes the minimum requirements for a team to win the challenge.\n\n### Submit an upstream model to the hidden-set leaderboard\n\nThe public-set is for the upstream development purpose. You can pre-train your upstream and evaluate it with any method you like. You are required to submit at least one upstream model to the hidden-set leaderboard. The hidden-set leaderboard submission deadline is **Jan 10, 2022**.\n\n### Submission selection\n\nA team can **select at most 2 submissions** among its previous submissions for the final team ranking: one for the parameter-agnostic metrics and another for the parameter-panelized metrics. However, these 2 submissions **must come from the same method** and only differ in parameter size. The deadline for the submission selection is **Jan 13, 2022**.\n\n### System description paper\n\nTo verify the submitted upstream follows the challenge policy, we require each team to submit a system description paper in **AAAI submission format** without the page limit. The paper should describe the method **for the selected submissions**, containing at least the following materials:\n\n- SSL objectives\n- Model architecture\n- Pre-training data\n- Parameter size for each submission\n\nSince all the selected submissions come from the same method, the above materials should be almost identical between submissions except for the parameter size.\n\nThe submission should follow the challenge policy and the paper is expected to be well-written. The deadline for the system description paper is **Jan 13, 2022**.\n\n#### Note 1.\n\nThe system description paper is for the challenge review only and is not considered as our AAAI workshop paper by default, since the [AAAI workshop has the early hard deadlines](https://aaai.org/Conferences/AAAI-22/ws22call/) for both paper submission (**Nov 12, 2021**) and acceptance/rejection announcement (**Dec 3, 2021**). Hence, **we encourage participants to submit their methods\' papers early to our AAAI workshop** before Nov 12, 2021. If the method turns out to be similar to that used for the final selected submissions, the same paper can be used as the system description paper.\n\n#### Note 2.\n\nWe plan to work with other conferences and offer presentation & paper submission opportunities.\n\n## Winner Announcement and Presentation\n\nAfter review the system description papers and compare their performance with the hidden-set **private scores**. We will reveal all the private scores and announce the final winners on **January 20, 2022**. The winners will be invited to present their methods in our AAAI workshop.\n\n## Timeline\n\n- Sep 18, 2021: Challenge announcement & [S3PRL](https://github.com/s3prl/s3prl/blob/master/s3prl/downstream/docs/superb.md) released\n- Sep 30, 2021: [Overall metrics](challenge#Overall-Metrics) announcement & [public-set leaderboard](leaderboard?track=constrained&subset=Public+Set) is online and [accepts submissions](submit?type=public)\n- Oct 15, 2021: [Hidden-set leaderboard](leaderboard?track=constrained&subset=Hidden+Dev+Set) is online and [accepts submissions](submit?type=hidden)\n- Nov 12, 2021: [AAAI workshop](https://aaai-sas-2022.github.io/) paper submission deadline (encouraged)\n- Dec 3, 2021: [AAAI workshop](https://aaai-sas-2022.github.io/) paper acceptance / rejection announcement\n- Jan 10, 2022: Hidden-set leaderboard submission deadline\n- Jan 13, 2022: Submission selection & system description paper deadline\n- Jan 20, 2022: Winner announcement & reveal hidden-set private scores\n- Jan 22, 2022: AAAI late [registration](https://aaai.org/Conferences/AAAI-21/registration/) deadline\n- Feb 28 - Mar 1, 2022: [AAAI workshop](https://aaai-sas-2022.github.io/) presentation\n\n## Organizers\n\nHung-yi Lee\n\nShinji Watanabe\n\nAbdelrahman Mohamed\n\nShang-Wen Li\n\nShuyan Dong\n\nHeng-Jui Chang\n\nHsuan-Jui Chen\n\nPo-Han Chi\n\nXuankai Chang\n\nYung-Sung Chuang\n\nTzu-Hsun Feng\n\nTzu-Hsien Huang\n\nWen-Chin Huang\n\nZili Huang\n\nAndy T. Liu\n\nCheng-I Jeff Lai\n\nGuan-Ting Lin\n\nKushal Lakhotia\n\nYist Y. Lin\n\nYassin Omar\n\nJiatong Shi\n\nHsiang-Sheng Tsai\n\nLewis Tunstall\n\nWei-Cheng Tseng\n\nShu-wen Yang\n\n## Hidden-set Committee\n\nXuankai Chang\n\nHsuan-Jui Chen\n\nYung-Sung Chuang\n\nZili Huang\n\nShang-Wen Li\n\nGuan-Ting Lin\n\nYassin Omar\n\nJiatong Shi\n\nHsiang-Sheng Tsai\n\nShu-wen Yang\n\n# Contact\n\nsuperb.announcement@gmail.com\n'],['\n# SUPERB Challenge\n\n## Evaluation Framework\n\n### Background\n\n![](https://i.imgur.com/FDARwvz.png)\n*Fig 1.*\n\nSUPERB Challenge follows the similar evaluation framework introduced in [SUPERB Benchmark](https://arxiv.org/abs/2105.01051), which benchmarks the **generalizability** of Self-Supervised Learning (SSL) on speech. SSL models are termed **Upstream** and are evaluated with various **Downstream** tasks. The framework extract **multiple frozen hidden states** from a single upstream model and trains a learnable **weighted-sum** over the hidden states along with the downstream model task-by-task.\n\n### Overview\n\n![](https://i.imgur.com/BNr2gfE.png)\n*Fig 2.*\n\nFig 2. illustrates the evaluation framework of the challenge. The challenge evaluates SSL models\' generalizability on 10 tasks. Each of the tasks has a corresponding public dataset (**public-set**) that is publicly available, and a hidden dataset (**hidden-set**) that will not be released. Participants can practice on the public-set to understand the performance of their upstream models, and choose the best one for submission as they wish. Then, participants **submit the upstream model** (model definition & pre-trained weights) publicly or privately to the hidden-set leaderboard. **We finetune the downstream models on the hidden-set** without releasing any audio/label. Both public-set and hidden-set have leaderboards and welcome submissions to share more results with the community. **The winners of the challenge will be solely determined by the ranking on the hidden-set leaderboard.** Finally, there will be **overall metrics** for ranking all upstreams.\n\nAll the participants are encouraged to submit papers to [*AAAI workshop: The 2nd Self-supervised Learning for Audio and Speech Processing*](https://aaai-sas-2022.github.io/). The winners of the challenge will be invited to present their methods in the workshop. We plan to collaborate with more conferences for participants to present their works and papers.\n\n### Tasks\n\n10 evaluation tasks are included in this challenge:\n\n- **Content**\n    - Phoneme Recognition (PR)\n    - Automatic Speech Recognition (ASR)\n    - Query-by-example Spoken Term Detection (QbE)\n- **Speaker**\n    - Speaker Identification (SID)\n    - Automatic Speaker Verification (ASV)\n    - Speaker Diarization (SD)\n- **Paralinguistics**\n    - Emotion Recognition (ER)\n- **Semantics**\n    - Speech Translation (ST)\n- **Generation**\n    - Speech Enhancement (SE)\n    - Speech Separation (SS)\n\nMore task descriptions for the public-set can be found in [TASKS](https://superbbenchmark.org/tasks), and we implement the evaluation scripts for public-set in [S3PRL](https://github.com/s3prl/s3prl/blob/master/s3prl/downstream/docs/superb.md) for reference. The task design and evaluation pipeline will be the same between public-set & hidden-set unless otherwise mentioned.\n\n### Secret tasks\n\nSecret tasks evaluate SSL models\' generalizability on completely unseen tasks. Secret tasks are only present in the hidden-set, and the task design will not be revealed until the final winner announcement. \n\n### What is new\n\nCompared with SUPERB Benchmark, SUPERB Challenge extends the framework with the following:\n\n- **New Tasks**: Speech Translation, Speech Enhancement, Source Separation and secret tasks.\n- **New Data Domains**: A challenging and newly recorded hidden-set with unseen (to upstream) text/audio domain.\n- **New Overall Metrics**: The metrics to rank upstreams.\n\n\n## Upstream Specification\n\n### Unlabeled data only: Focus on SSL\n\n- Any labeled/parallel data made by human annotators are **not allowed** to used for both model training and data preprocessing, e.g.\n    - **audio/text pairs:** transcriptions in English, foreign languages, or phonemes.\n    - **audio/tagging pairs:** speaker labels or sound event labels.\n    - **audio/audio pairs:** audios with the same properties made parallel by human, e.g. audios with same content from different speakers, or the opposite.\n- Any system pre-trained by labeled/parallel data **cannot** be used to help with the SSL pre-training, like pre-trained ASR.\n- Any unlabeled/unparallel data is allowed, including the downstream datasets in the public-set. The nature alignments (not made by human annotators) bettwen audio and other modalities are also allowed, e.g. videos.\n- If it is hard to define whether your data is labeled/parallel, please [contact us](#Contact)!\n\n### Programming Language\n\n- We currently support:\n    - **Python >= 3.6**\n    - **Pytorch >= 1.7**\n\n- We expect the upstream submission can pass the following check:\n    ~~~python=\n    upstream = YourModel.cuda()\n    assert isinstance(upstream, torch.nn.Module)\n    ~~~\n\n\nWe accept upstream models in PyTorch by default. If you wish to submit upstreams in non-PyTorch frameworks, please [fill this form](https://docs.google.com/forms/d/e/1FAIpQLSe52jYL2Yk9oYqXfg_Bg0Sjp01a6HSLUhY5VohsZOE5sNmgsw/viewform)!\nIf you are not feasible to submit the pre-trained model, please [fill this form](https://docs.google.com/forms/d/e/1FAIpQLSdA44nArlIDfGV63WwtwXer4WAPQO1aBwEpAjDSNjbMQN-GJQ/viewform) for us to see how to help!\n\n### Interface functions\n\n#### forward\n\nExtract features from waveforms.\n\n- **Input:** A list of waveforms in 16000 Hz\n\n    ~~~python=\n    SAMPLE_RATE = 16000\n    BATCH_SIZE = 8\n    EXAMPLE_SEC = 10\n    wavs = [torch.randn(SAMPLE_RATE * EXAMPLE_SEC).cuda() for _ in range(BATCH_SIZE)]\n    results = upstream(wavs)\n    ~~~\n\n- **Output:** A dictionary with a key for each task, and a single key for all secret tasks. If any task-specific key is not presented, a "hidden_states" key should be provided as the default key. The value for each key is **a list** of padded sequences in the same shape of **(batch_size, max_sequence_length_of_batch, hidden_size)** for weighted-sum to work. It is welcomed to perform some preprocessing on the upstream\'s raw hidden-sets, including upsampling and downsampling. However, all the values must come from **a single upstream model**:\n\n    ~~~python=\n    assert isinstance(results, dict)\n    tasks = ["PR", "SID", "ER", "ASR", "ASV", "SD", "QbE", "ST", "SS", "SE", "secret"]\n    for task in tasks:\n        hidden_states = results.get(task, "hidden_states")\n        assert isinstance(hidden_states, list)\n\n        for state in hidden_states:\n            assert isinstance(state, torch.Tensor)\n            assert state.dim() == 3, "(batch_size, max_sequence_length_of_batch, hidden_size)"\n            assert state.shape == hidden_states[0].shape\n    ~~~\n\n#### get_downsample_rates\n\nProvide the downsample rate **from 16000 Hz waveforms** for each task\'s representation in the dict. For the standard 10ms stride representation, the downsample rate is 160.\n\n~~~python=\nSAMPLE_RATE = 16000\nMSEC_PER_SEC = 1000\ndownsample_rate = SAMPLE_RATE * 10 / MSEC_PER_SEC  # 160\n~~~\n\nThe downsample rate will be used to:\n\n1. Calculate the valid representation length of each utterance in the output padded representation.\n2. Prepare the training materials according to the representation\'s downsample rate for frame-level tasks: SD, SE, SS.\n\n- **Input:** the task key (str)\n- **Output:** the downsample rate (int) of the representation for that task\n\n~~~python=\nfor task in tasks:\n    assert isinstance(task, str)\n    downsample_rate = upstream.get_downsample_rate(task)\n    assert isinstance(downsample_rate, int)\n    print("The upstream\'s representation for {task}"\n        f" has the downsample rate of {downsample_rate}.")\n~~~\n\n## Public-set and S3PRL toolkit\n\n### As the task definition and demonstration\n\nThe public-set serves as the demonstration of the task design: including the data preprocessing, tasks\' input/output formats and task-specific metrics. The datasets used in the public-set are all chosen to be public available for everyone to participate. Please refer to [TASKS](https://superbbenchmark.org/tasks) and the implementation in [S3PRL](https://github.com/s3prl/s3prl/blob/master/s3prl/downstream/docs/superb.md) for details.\n\n### As the platform for developing upstreams for the hidden-set\n\nThe differences between the public-set and the hidden-set are controlled to be only the following:\n\n1. Recording conditions\n2. Spoken content / text scripts\n3. Speakers\n4. Fewer labeled data\n\nIn this way, the public-set is still a good indicator of the hidden-set performance to some degree. We follow the same (unless mentioned otherwise in [TASKS](/tasks)) implementation in the public-set for the hidden-set, and hence encourage participants to use [S3PRL](https://github.com/s3prl/s3prl/blob/master/s3prl/downstream/docs/superb.md) to benchmark their upstream models (optional) on the public-set. The winners of the challenge will be decided solely on the hidden-set, and the public-set and [S3PRL](https://github.com/s3prl/s3prl/blob/master/s3prl/downstream/docs/superb.md) implementations can serve as the start-kit.\n\n### Provide baselines for comparison\n\n#### Baselines\n\nWe collected most of the well-known SSL baseline models in [S3PRL](https://github.com/s3prl/s3prl/blob/master/s3prl/downstream/docs/superb.md), including TERA, wav2vec2, Hubert, DeCoAR 2.0, and more. You can easily benchmark different upstreams by specifying in the command line arguments.\n\n#### Comparison\n\nSince the full benchmarking on the public-set can take some time for the training to converge. We released the [training artifacts](https://github.com/s3prl/s3prl/blob/master/s3prl/downstream/docs/superb_artifacts.md) of the top baseline systems (e.g. wav2vec2, HuBERT) for participants to quickly compare with them. The artifacts include:\n\n- Tensorboard logs\n- Trained downstream weights (the best on public dev set)\n\n### Public-set leaderboard and submission\n\nThe [public-set leaderboard](leaderboard?subset=Public+Set&track=constrained) is online and [accepts submissions](submit?type=public). There is no deadline. Since all the train/dev/test splits are public available, **the leaderboard accepts submissions with the inferenced prediction files on each task\'s testing split** which will be auto-generated if you follow the benchmarking steps in [S3PRL](https://github.com/s3prl/s3prl/blob/master/s3prl/downstream/docs/superb.md).\n\n## Hidden-set\n\n### Fairness & prevent overfitting\n\nSince all the train/dev/test splits are public in the public-set, it is possible to cheat by directly reporting the best results on the testing split, and the results are thus overfit on the testing split. Hence, the hidden-set is collected and prepared to follow the same task design as that in the public-set but with the newly created data. All the splits will **NOT be released in both audio and labels**. The members involved in the hidden-set preparation should **NOT** participate the challenge. These members are listed in the **Hidden-set Committee** below.\n\n### Hidden-set leaderboard and submission\n\n#### Submission type\n\nThe leaderboard accepts **submissions with the upstream model solely**, including **model definition** and **pre-trained weights**. The upstream model should follow the specification detailed at [Upstream Specification](#Upstream-Specification). The submission can be done publicly or privately. Only the **Hidden-set Committee** members can access the privately submitted upstreams and the models will be used solely for this challenge.\n\n- We accept upstream models in PyTorch by default. If you wish to submit upstreams in non-PyTorch frameworks, please [fill this form](https://docs.google.com/forms/d/e/1FAIpQLSe52jYL2Yk9oYqXfg_Bg0Sjp01a6HSLUhY5VohsZOE5sNmgsw/viewform)!\n- If you are not feasible to submit the pre-trained model, please [fill this form](https://docs.google.com/forms/d/e/1FAIpQLSdA44nArlIDfGV63WwtwXer4WAPQO1aBwEpAjDSNjbMQN-GJQ/viewform) for us to see how to help!\n\n#### Finetuning on submission\n\nAfter the upstream model is submitted, we **benchmark the submitted upstream by finetuning each task\'s downstream model for participants**. **The quota for submissions per week is limited and starts from 2 times/week,** but will be dynamically adjusted based on the number of participants. The quota adjustment will be announced at [NEWS](https://superbbenchmark.org/news). Participants can [contact us](#Contact) to acquire the finetuning artifacts of their own submissions for sanity checks, including:\n\n- Tensorboard logs\n- Testing results\n- Trained downstream weights\n\n#### Practice (dev) / Private (test) scores\n\nAfter training the downstream model for all tasks, we show the best performance on the hidden-set\'s development splits as the **practice scores** (one score per task). The true performance on the testing splits, termed **private scores**, will be revealed along with the final winner announcement. The final team ranking will depend only on the hidden-set\'s **private scores**.\n\n#### How to submit\n\n(Update 10/20/2021 AOE)\n\nThe [hidden-set leaderboard](leaderboard?subset=Hidden+Dev+Set&track=constrained) is online and accepts submissions. Please follow [the submission steps](https://huggingface.co/superb/superb-submission). We use HuggingFace\'s Hub to host the submitted upstreams and track the submitted model weights. On the other hand, we use our [submission page](submit?type=hidden) to control the submission limit, where the participants tell us the locations of their models on HuggingFace\'s Hub.\n\nAfter submitting via the submission page, you will see a new entry in the **submission history** under your Profile page, and the evaluation results on the hidden-set will be revealed at the entry within one week.\nPlease stay tuned!\n\n(Update 10/31/2021 AOE)\n\nAll the submitted results will remain anonymous during and after the challenge.\nYou can select a few submissions to show on the leaderboard.\nIf no submission is selected, we will show all your submissions on the leaderboard anonymously.\nOnly you can see your personal/model information on the leaderboard, but you will not see others\'.\nIf you wish to reveal your personal/model information on the [hidden-set leaderboard](leaderboard?subset=Hidden+Dev+Set&track=constrained), please [contact us](#Contact)!\n\n## Overall Metrics\n\n(Update 10/1/2021 AOE)\n\nWe announce two kinds of metrics: **superb-rank** and **superb-score**, each with parameter-agnostic and parameter-panelized versions.\n\nType|Parameter-agnostic|Parameter-penalized\n-|-|-\nScoring|$superb_s$|$superb_{sp}$\nRanking|$superb_r$|$superb_{rp}$\n\nIn this challenge, $superb_r$ and $superb_{rp}$ are the primary metrics. When equal rank is found on two different upstreams, $superb_{s}$ or $superb_{sp}$ is used to break the tie. Hence, there will be only two final lists of winners: **Parameter-agnostic** and **Parameter-penalized**.\n\n### Notation\n\nSuppose each task $t$ in all tasks $T$ has a single metric $s_t$, and the score of an upstream $u$ on task $t$ is $s_t(u)$ which has already been transformed to make higher values represent better performance. Eg. We use WAcc here for ASR instead of the raw WER. The upstream $u$ has $|u|$ millions parameters.\n\n### Parameter-agnostic\n\nParameter-agnostic metrics demonstrate the best performance SSL can achieve, and encourages participants to explore any possibility to push the limits.\n\n#### $superb_s$\n\nTo aggregate all task-specific scores $s_t(u)$ into a single static score, we linearly transform each of them into points so that:\n\n- $s_t(fbank) = 0$, The performance of FBANK maps to 0\n- $s_t(sota) = 1000$, The performance of the existing SOTA upstream *for this task* maps to 1000.\n\n$$\np_t(u) = \\dfrac{1000}{s_t({sota}) - s_t({fbank})}\\ (\\ s_t(u) - s_t({fbank})\\ )\n$$\n\nHence, most of the points will sit between 0~1000. The upstream worse than FBANK on this task will get negative points. The upstream better than the task-specific SOTA upstream will get points higher than 1000. The $superb_s$ of the upstream $u$ is the average of $p_t$ over all tasks.\n\n$$\nsuperb_s = \\dfrac{1}{|T|} \\sum_{t \\in T}\\ p_t(u)\n$$\n\nIntuitively, two reference points: FBANK and SOTA decide the typical interested interval for a task-specific metric and scale the task scores to 0~1000 points accordingly. The similar range of points across tasks can then be averaged. Beyond scaling with the pre-defined metric range, this interval further determines *how hard for a task to improve*. For a harder task, its smaller interval at the denominator give the task more credit for any unit improvement.\n\n#### $superb_r$\n\nTo encourage the development on **universal models** instead of models skewed toward a subset of tasks. We use ranking to saturate the improvement when an upstream already become the best for that task. The $superb_r$ for an upstream $u$ is the average number of upstreams which $u$ can win in each task. This metric dynamically depends on all the upsreams $U$ shown on the leaderboard. In the following, $L$ is the number of upstreams which $u$ can win using the metric $x_t$.\n\n$$\nL(x_t, u) = |\\ \\{\\ \\hat{u} \\in U\\ |\\ x_t(u) > x_t(\\hat{u}) \\ \\}\\ |\n$$\n\n$$\nsuperb_r = \\dfrac{1}{|T|} \\sum_{t \\in T} L(s_t, u) = \\dfrac{1}{|T|} \\sum_{t \\in T} L(p_t, u)\n$$\n\n### Parameter-penalized\n\nTo encourage the development of speech SSL on small and green models, we add a metric with penalization on models\' parameter size. To gauge the effectiveness and parameter-efficiency of new SSL algorithms, we encourage participants to submit multiple upstreams trained by the same algorithm with the only difference in parameter size.\n\n#### $superb_{sp}$\n\nWe penalize the parameter-agnostic scoring by an upstream $u$\'s parameter size $|u|$.\n\n$$\n\\hat{p_t}(u) =\n\\left\\{\n    \\begin{array}{lr}\n        \\frac{p_t(u)}{\\ max(|u|,\\ 1)},& \\text{if } p_t(u) \\geq 0\\\\\n        \\ \\ \\ \\ p_t(u),              & \\text{otherwise}\n    \\end{array}\n\\right.\n$$\n\nSince either the point of the baseline FBANK or its parameter size is 0. The above formula measures how many improvement upon FBANK per parameter. The minimum 1 million parameter size is designed to avoid too small upstreams dominating others by receiving too much credit on a single task. When $p_t(u)$ is negative we do not penalize it, since it is already worse than the zero-parameter FBANK.\n\n$$\nsuperb_{sp} = \\dfrac{1}{|T|} \\sum_{t \\in T}\\ \\hat{p_t}(u)\n$$\n\n#### $superb_{sp}$\n\nSimilar to $superb_r$, we rank all upstreams on the leaderboard with $\\hat{p_t}$.\n\n$$\nsuperb_{rp} = \\dfrac{1}{|T|} \\sum_{t \\in T}\\ L(\\hat{p_t}, u)\n$$\n\n### Conclusion\n\nThe ranking metrics are the primary measures in this challenge and are designed to emphasize an upstream\'s universally usability. When two upstreams tie on the same rank, the scoring metrics take the tasks\' variations and improvement difficulty into account to help the final decision. You can refer to the [public-set leaderboard](leaderboard?subset=Public+Set&track=constrained) for the overal metrics calculation.\n\n#### Reference points\n\nIf a task have multiple metrics, each metric is first tranformed into points or ranks as illustrated above. Then, points or ranks are first averaged in intra-task fashion before being averaged with other task.\n\nTask|PR|SID|ER|ASR|QbE|ASV|SD|ST|SE|SE|SS\n-|-|-|-|-|-|-|-|-|-|-|-\nMetrics|PER|ACC|ACC|WER|MTWV|EER|DER|BLEU|PESQ|STOI|SS\nFBANK|82.01|41.38|48.24|23.18|0.58|9.56|10.05|2.32|2.55|0.9364|9.234\nSOTA|3.53|96.66|67.62|3.62|7.36|5.62|5.11|20.01|2.64|0.9418|10.45\n\n## Winner Minimum Requirements\n\nThe following describes the minimum requirements for a team to win the challenge.\n\n### Submit an upstream model to the hidden-set leaderboard\n\nThe public-set is for the upstream development purpose. You can pre-train your upstream and evaluate it with any method you like. You are required to submit at least one upstream model to the hidden-set leaderboard. The hidden-set leaderboard submission deadline is **Jan 10, 2022**.\n\n### Submission selection\n\nA team can **select at most 2 submissions** among its previous submissions for the final team ranking: one for the parameter-agnostic metrics and another for the parameter-panelized metrics. However, these 2 submissions **must come from the same method** and only differ in parameter size. The deadline for the submission selection is **Jan 13, 2022**.\n\n### System description paper\n\nTo verify the submitted upstream follows the challenge policy, we require each team to submit a system description paper in **AAAI submission format** without the page limit. The paper should describe the method **for the selected submissions**, containing at least the following materials:\n\n- SSL objectives\n- Model architecture\n- Pre-training data\n- Parameter size for each submission\n\nSince all the selected submissions come from the same method, the above materials should be almost identical between submissions except for the parameter size.\n\nThe submission should follow the challenge policy and the paper is expected to be well-written. The deadline for the system description paper is **Jan 13, 2022**.\n\n#### Note 1.\n\nThe system description paper is for the challenge review only and is not considered as our AAAI workshop paper by default, since the [AAAI workshop has the early hard deadlines](https://aaai.org/Conferences/AAAI-22/ws22call/) for both paper submission (**Nov 12, 2021**) and acceptance/rejection announcement (**Dec 3, 2021**). Hence, **we encourage participants to submit their methods\' papers early to our AAAI workshop** before Nov 12, 2021. If the method turns out to be similar to that used for the final selected submissions, the same paper can be used as the system description paper.\n\n#### Note 2.\n\nWe plan to work with other conferences and offer presentation & paper submission opportunities.\n\n## Winner Announcement and Presentation\n\nAfter review the system description papers and compare their performance with the hidden-set **private scores**. We will reveal all the private scores and announce the final winners on **January 20, 2022**. The winners will be invited to present their methods in our AAAI workshop.\n\n## Timeline\n\n- Sep 18, 2021: Challenge announcement & [S3PRL](https://github.com/s3prl/s3prl/blob/master/s3prl/downstream/docs/superb.md) released\n- Sep 30, 2021: [Overall metrics](challenge#Overall-Metrics) announcement & [public-set leaderboard](leaderboard?track=constrained&subset=Public+Set) is online and [accepts submissions](submit?type=public)\n- Oct 15, 2021: [Hidden-set leaderboard](leaderboard?track=constrained&subset=Hidden+Dev+Set) is online and [accepts submissions](submit?type=hidden)\n- Nov 12, 2021: [AAAI workshop](https://aaai-sas-2022.github.io/) paper submission deadline (encouraged)\n- Dec 3, 2021: [AAAI workshop](https://aaai-sas-2022.github.io/) paper acceptance / rejection announcement\n- Jan 10, 2022: Hidden-set leaderboard submission deadline\n- Jan 13, 2022: Submission selection & system description paper deadline\n- Jan 20, 2022: Winner announcement & reveal hidden-set private scores\n- Jan 22, 2022: AAAI late [registration](https://aaai.org/Conferences/AAAI-21/registration/) deadline\n- Feb 28 - Mar 1, 2022: [AAAI workshop](https://aaai-sas-2022.github.io/) presentation\n\n## Organizers\n\nHung-yi Lee\n\nShinji Watanabe\n\nAbdelrahman Mohamed\n\nShang-Wen Li\n\nShuyan Dong\n\nHeng-Jui Chang\n\nHsuan-Jui Chen\n\nPo-Han Chi\n\nXuankai Chang\n\nYung-Sung Chuang\n\nTzu-Hsun Feng\n\nTzu-Hsien Huang\n\nWen-Chin Huang\n\nZili Huang\n\nAndy T. Liu\n\nCheng-I Jeff Lai\n\nGuan-Ting Lin\n\nKushal Lakhotia\n\nYist Y. Lin\n\nYassin Omar\n\nJiatong Shi\n\nHsiang-Sheng Tsai\n\nLewis Tunstall\n\nWei-Cheng Tseng\n\nShu-wen Yang\n\n## Hidden-set Committee\n\nXuankai Chang\n\nHsuan-Jui Chen\n\nYung-Sung Chuang\n\nZili Huang\n\nShang-Wen Li\n\nGuan-Ting Lin\n\nYassin Omar\n\nJiatong Shi\n\nHsiang-Sheng Tsai\n\nShu-wen Yang\n\n# Contact\n\nsuperb.announcement@gmail.com\n'])));Object(h.a)((e=>({ProSidebar:{position:"fixed",top:"0px",left:"0px","padding-top":"60px",color:"#aaa","background-color":"#aeaeae",boxShadow:"2px 3px 5px rgba(0, 0, 0, 0.1)"},BtnToggle:{position:"fixed",top:"80px",left:"20px",fontSize:"20px"}})));function Ne(){const e=Object(b.a)(),t=te("https://raw.githubusercontent.com/dynamic-superb/dynamic-superb/main/docs/call_for_tasks.md");return Object(f.jsx)(p.a,{margin:e.spacing(8,0),textAlign:"center",children:Object(f.jsx)(he,{mdFile:t})})}function ze(){const e=Object(b.a)(),t=te("https://raw.githubusercontent.com/dynamic-superb/dynamic-superb/main/docs/review_process.md");return Object(f.jsx)(p.a,{margin:e.spacing(8,0),textAlign:"center",children:Object(f.jsx)(he,{mdFile:t})})}var He=n(1167),Fe=n(1165),Ve=n(1168),Ue=n(506),Je=n(1169),Ke=n(1031),Ye=n(1170),Qe=n(1171),qe=n(1172),Xe=n(1173),Ze=n(1175),et=n(494),tt=n.n(et),nt=n(1188),it=n(1174),at=n(5),st=n(1158),rt=n(1187);const ot=Object(h.a)({list:{width:200,paddingLeft:20,paddingRight:20},fullList:{width:"auto"}});function ct(e){const t=ot(),[n,i]=a.a.useState({top:!1,left:!1,bottom:!1,right:!1}),s=(e,t)=>a=>{(!a||"keydown"!==a.type||"Tab"!==a.key&&"Shift"!==a.key)&&i({...n,[e]:t})},r=n=>Object(f.jsx)("div",{className:Object(at.a)(t.list,{[t.fullList]:"top"===n||"bottom"===n}),role:"presentation",onKeyDown:s(n,!1),children:Object(f.jsx)(st.a,{children:e.items(s(n,!1))})});return Object(f.jsx)("div",{children:["right"].map((t=>Object(f.jsxs)(a.a.Fragment,{children:[Object(f.jsx)("div",{onClick:s(t,!0),children:e.children}),Object(f.jsx)(rt.a,{anchor:t,open:n[t],onClose:s(t,!1),onOpen:s(t,!0),children:r(t)})]},t)))})}function ht(e){const{children:t,window:n}=e,i=Object(He.a)({target:n?n():void 0});return Object(f.jsx)(Fe.a,{appear:!1,direction:"down",in:!i,children:t})}function lt(e){const{children:t,window:n}=e,i=dt();let a=Object(o.g)(),s=Object(o.h)();const r=Object(He.a)({target:n?n():void 0,disableHysteresis:!0,threshold:100});return Object(f.jsx)(Ve.a,{in:r,children:Object(f.jsx)("div",{onClick:e=>{const t=(e.target.ownerDocument||document).querySelector("#back-to-top-anchor");t&&(t.scrollIntoView({block:"center"}),a.push(s.pathname))},role:"presentation",className:i.root,children:t})})}const dt=Object(h.a)((e=>({root:{position:"fixed",bottom:e.spacing(2),right:e.spacing(2)},toolbar:{width:"100%",maxWidth:1600,margin:"auto",paddingLeft:20,paddingRight:20},tool:{paddingTop:5,paddingBottom:5},button:{paddingLeft:12,paddingRight:12,marginLeft:2,marginRight:2,borderRadius:10,border:"solid 1px transparent","&:hover":{background:"rgba(255, 255, 255, 1)",boxShadow:"0px 3px 10px rgba(0, 0, 0, 0.1)"},cursor:"pointer"},navlink:{fontWeight:"inherit"}})));function pt(e){const t=dt();return Object(f.jsx)("div",{className:"".concat(t.tool," ").concat(t.button),children:e.children})}var mt=Object(it.a)()((function(e){let{width:t,tableControlRef:n,...s}=e;const r=dt(),c=Object(b.a)(),h=Object(o.h)(),[l,d]=(Object(i.useContext)(be),a.a.useState(null)),S=e=>{d(e.currentTarget)},L=()=>{d(null)};let w=[["Call for Task","/call-for-task"],["Submission",[["Task Submission","/task-submission"],["Task Proposal","/task-proposal"],["Review Process","/review-process"],["Score Submission","/score-submission"]]],["Tasks","/tasks"],["Leaderboard","/leaderboard"],["Github","https://github.com/dynamic-superb/dynamic-superb"],["Paper","https://arxiv.org/abs/2309.09510"]];const T=w.map((e=>{let[t,n]=e;return Array.isArray(n)?Object(f.jsxs)(p.a,{sx:{flexGrow:0},children:[Object(f.jsx)(u.a,{item:!0,onClick:S,children:Object(f.jsx)(pt,{children:Object(f.jsx)(m.a,{color:"textSecondary",variant:"overline",className:r.navlink,children:t},t)})}),Object(f.jsx)(Ue.a,{sx:{mt:"45px"},id:"menu-appbar",anchorEl:l,anchorOrigin:{vertical:"top",horizontal:"right"},keepMounted:!0,transformOrigin:{vertical:-50,horizontal:"right"},open:Boolean(l),onClose:L,onMouseLeave:L,children:n.map((e=>{let[t,n]=e;return Object(f.jsx)(Je.a,{onClick:L,children:Object(f.jsx)(M,{link:n,children:Object(f.jsx)(m.a,{textAlign:"center",children:t})})},t)}))})]}):Object(f.jsx)(u.a,{item:!0,children:Object(f.jsx)(M,{link:n,children:Object(f.jsx)(pt,{children:Object(f.jsx)(m.a,{color:"textSecondary",variant:"overline",className:r.navlink,children:t},t)})})},n)}));return Object(g.a)(c.breakpoints.up("lg")),Object(f.jsxs)(a.a.Fragment,{children:[Object(f.jsx)(Qe.a,{}),Object(f.jsx)(ht,{...s,children:Object(f.jsx)(qe.a,{color:"primary",children:Object(f.jsx)(Xe.a,{className:r.toolbar,children:Object(f.jsxs)(u.a,{container:!0,alignItems:"center",children:[Object(f.jsx)(u.a,{item:!0,xs:9,lg:2,children:Object(f.jsxs)(u.a,{container:!0,direction:"row",justify:"flex-start",alignItems:"center",spacing:1,children:[Object(f.jsx)(u.a,{item:!0,children:Object(f.jsx)(M,{link:"/",children:Object(f.jsx)(pt,{children:Object(f.jsx)(m.a,{color:"textPrimary",variant:"h6",className:"".concat(r.navlink),children:"Dynamic-SUPERB"})})})}),(h.pathname.includes("leaderboard-tmp")||h.pathname.includes("profile"))&&Object(f.jsx)(u.a,{item:!0,children:Object(f.jsx)(M,{link:null,children:Object(f.jsx)(j.a,{size:"small",variant:"outlined",onClick:()=>{n.current.click()},children:"Help"})})})]})}),Object(f.jsx)(u.a,{item:!0,xs:3,lg:10,children:Object(f.jsxs)(u.a,{container:!0,direction:"row",justify:"flex-end",alignItems:"center",children:[Object(f.jsx)(nt.a,{mdDown:!0,children:T}),Object(f.jsx)(nt.a,{lgUp:!0,children:Object(f.jsx)(u.a,{item:!0,children:Object(f.jsx)(ct,{items:e=>w.map((t=>{let[n,i]=t;return Array.isArray(i)?Object(f.jsxs)(p.a,{sx:{flexGrow:0},children:[Object(f.jsx)(u.a,{item:!0,onClick:S,children:Object(f.jsx)(Ke.a,{button:!0,children:Object(f.jsx)(Ye.a,{children:Object(f.jsx)(m.a,{color:"textSecondary",variant:"overline",className:r.navlink,children:n},n)})})}),Object(f.jsx)(Ue.a,{sx:{mt:"45px",flexGrow:1},id:"menu-appbar",anchorEl:l,anchorOrigin:{vertical:"top",horizontal:"left"},keepMounted:!0,transformOrigin:{vertical:-50,horizontal:"left"},open:Boolean(l),onClose:L,onMouseLeave:L,children:i.map((t=>{let[n,i]=t;return Object(f.jsx)("div",{onClick:e,children:Object(f.jsx)(Je.a,{onClick:L,children:Object(f.jsx)(M,{link:i,children:Object(f.jsx)(m.a,{textAlign:"center",children:n})})},n)})}))}),Object(f.jsx)(y.a,{})]}):Object(f.jsxs)("div",{children:[Object(f.jsx)(M,{link:i,children:Object(f.jsx)(Ke.a,{button:!0,onClick:e,children:Object(f.jsx)(Ye.a,{children:Object(f.jsx)(m.a,{color:"textSecondary",variant:"overline",className:r.navlink,children:n},n)})})}),Object(f.jsx)(y.a,{})]})})),children:Object(f.jsx)(pt,{children:Object(f.jsx)(m.a,{color:"textSecondary",variant:"overline",children:"MENU"})})})})})]})})]})})})}),Object(f.jsx)(Xe.a,{id:"back-to-top-anchor"}),Object(f.jsx)(lt,{...s,children:Object(f.jsx)(Ze.a,{color:"secondary",size:"small","aria-label":"scroll back to top",children:Object(f.jsx)(tt.a,{})})})]})})),ut=n(248);var bt=n(496);n(187),n(495),n(315),n(500),n(501),n(503),n(502),n(1185),n(1166),n(1030),n(1176);Object(h.a)((e=>({modal:{display:"flex",alignItems:"center",justifyContent:"center"}})));var gt;n(1157),n(1154),n(1184);bt.a.div(gt||(gt=Object($e.a)(["\n  .table {\n    outline: 1px solid #ddd;\n\n    .click-btn {\n      cursor: pointer;\n    }\n    \n    .th,\n    .td {\n      background-color: ",";\n      overflow: hidden;\n      white-space: nowrap;\n      text-overflow: ellipsis;\n      border: 0.2px solid #ddd;\n      vertical-align: middle;\n      padding: ",";\n    }\n\n    .th {\n      font-weight: bold;\n      padding: ",";\n    }\n\n    .toggle {\n      display: flex;\n      align-items: center;\n      justify-content: center;\n    }\n\n    &.sticky {\n      overflow: scroll;\n      .header,\n      .footer {\n        position: sticky;\n        z-index: 1;\n        width: fit-content;\n      }\n\n      .header {\n        top: 0;\n        box-shadow: 0px 3px 3px #ccc;\n      }\n\n      .footer {\n        bottom: 0;\n        box-shadow: 0px -3px 3px #ccc;\n      }\n\n      .body {\n        position: relative;\n        z-index: 0;\n      }\n\n      [data-sticky-td] {\n        position: sticky;\n      }\n\n      [data-sticky-last-left-td] {\n        box-shadow: 2px 2px 3px #ccc;\n      }\n\n      [data-sticky-first-right-td] {\n        box-shadow: -2px -2px 3px #ccc;\n      }\n    }\n  }\n\n  .resizer {\n    display: inline-block;\n    background: ",";\n    width: ",";\n    height: 100%;\n    position: absolute;\n    right: 0;\n    top: 0;\n    transform: translateX(50%);\n    z-index: 1;\n    ","\n    touch-action:none;\n\n    &.isResizing, &:hover {\n      background: ",";\n    }\n"])),(e=>e.theme.palette.primary.main),(e=>e.theme.spacing(1,1)),(e=>e.theme.spacing(1.5,1)),(e=>"".concat(Object(k.c)(e.theme.palette.text.primary,.2))),(e=>"".concat(e.theme.spacing(2),"px")),"",(e=>"".concat(Object(k.c)(e.theme.palette.text.primary,.6))));let St;function ft(){const e=Object(b.a)(),t=te("https://raw.githubusercontent.com/dynamic-superb/dynamic-superb/main/docs/task_submission.md");return Object(f.jsx)(p.a,{margin:e.spacing(8,0),textAlign:"center",children:Object(f.jsx)(he,{mdFile:t})})}function Lt(){const e=Object(b.a)(),t=te("https://raw.githubusercontent.com/dynamic-superb/dynamic-superb/main/docs/score_submission.md");return Object(f.jsx)(p.a,{margin:e.spacing(8,0),textAlign:"center",children:Object(f.jsx)(he,{mdFile:t})})}function wt(){let{name:e}=Object(o.i)();const t=Object(b.a)(),n=te("/benchmark_tasks/".concat(e,"/README.md"));return Object(f.jsx)(p.a,{margin:t.spacing(8,0),textAlign:"center",children:Object(f.jsx)(he,{mdFile:n})})}function Tt(){const e=Object(b.a)(),t=te("https://raw.githubusercontent.com/dynamic-superb/dynamic-superb/main/docs/task_proposal.md");return Object(f.jsx)(p.a,{margin:e.spacing(8,0),textAlign:"center",children:Object(f.jsx)(he,{mdFile:t})})}const kt=Object(h.a)((e=>({narrowViewport:{width:"85%",maxWidth:900,margin:"auto"},LoginButton:{height:"30vh",paddingTop:"10vh"}})));function jt(){const[e,t]=a.a.useState(0),[n,i]=a.a.useState(0),[s,r]=a.a.useState(0),h=a.a.useRef(null),l=()=>{t(window.innerWidth),i(window.innerHeight),r(document.getElementById("navbar").offsetHeight)};a.a.useEffect(l),window.addEventListener("resize",l);const d=kt();let p=Object(f.jsxs)(o.d,{children:[Object(f.jsx)(o.b,{path:"/",exact:!0,children:Object(f.jsx)("div",{className:"".concat(d.narrowViewport),children:Object(f.jsx)(le,{})})}),Object(f.jsx)(o.b,{path:"/tasks",children:Object(f.jsx)("div",{className:"".concat(d.narrowViewport),children:Object(f.jsx)(ue,{})})}),Object(f.jsx)(o.a,{from:"/task/",to:"/tasks",exact:!0}),Object(f.jsx)(o.b,{path:"/task/:name",children:Object(f.jsx)("div",{className:"".concat(d.narrowViewport),children:Object(f.jsx)(wt,{})})}),Object(f.jsx)(o.b,{path:"/call-for-task",children:Object(f.jsx)("div",{className:"".concat(d.narrowViewport),children:Object(f.jsx)(Ne,{})})}),Object(f.jsx)(o.b,{path:"/task-submission",children:Object(f.jsx)("div",{className:"".concat(d.narrowViewport),children:Object(f.jsx)(ft,{})})}),Object(f.jsx)(o.b,{path:"/score-submission",children:Object(f.jsx)("div",{className:"".concat(d.narrowViewport),children:Object(f.jsx)(Lt,{})})}),Object(f.jsx)(o.b,{path:"/review-process",children:Object(f.jsx)("div",{className:"".concat(d.narrowViewport),children:Object(f.jsx)(ze,{})})}),Object(f.jsx)(o.b,{path:"/task-proposal",children:Object(f.jsx)("div",{className:"".concat(d.narrowViewport),children:Object(f.jsx)(Tt,{})})}),Object(f.jsx)(o.b,{path:"/leaderboard",children:Object(f.jsx)("div",{children:Object(f.jsx)(Ge,{})})})]});return Object(f.jsx)("div",{className:"App",children:Object(f.jsxs)(c.a,{children:[Object(f.jsx)("div",{id:"top"}),Object(f.jsx)("div",{id:"navbar",children:Object(f.jsx)(mt,{tableControlRef:h})}),p]})})}var yt=()=>{const{token:e,isAdmin:t,email:n,login:a,logout:s}=(()=>{const[e,t]=Object(i.useState)(!1),[n,a]=Object(i.useState)(!1),[s,r]=Object(i.useState)(null),[o,c]=Object(i.useState)(),{signOut:h}=Object(ut.useGoogleLogout)({clientId:"796679159105-6335p2q2ub5pr15lnf3g2cqkhnucmvkl.apps.googleusercontent.com",onLogoutSuccess:()=>{},onFailure:()=>{}}),l=Object(i.useCallback)(((e,n,i,s)=>{const o=n||new Date((new Date).getTime()+36e5);i=i||!1,s=s||null,t(e),c(o),a(i),r(s),localStorage.setItem("data",JSON.stringify({token:e,expiration:o.toISOString(),isAdmin:i,email:s}))}),[]),d=Object(i.useCallback)((()=>{h(),t(null),c(null),a(!1),r(null),localStorage.removeItem("data")}),[]);return Object(i.useEffect)((()=>{if(e&&o){const e=o.getTime()-(new Date).getTime();St=setTimeout(d,e)}else clearTimeout(St)}),[e,d,o]),Object(i.useEffect)((()=>{const e=JSON.parse(localStorage.getItem("data"));e&&e.token&&new Date(e.expiration)>new Date&&e.email?l(e.token,new Date(e.expiration),e.isAdmin,e.email):d()}),[l]),{token:e,isAdmin:n,email:s,login:l,logout:d}})();return Object(f.jsx)(l.a,{theme:Object(d.a)(V),children:Object(f.jsx)(jt,{})})};var vt=e=>{e&&e instanceof Function&&n.e(3).then(n.bind(null,1195)).then((t=>{let{getCLS:n,getFID:i,getFCP:a,getLCP:s,getTTFB:r}=t;n(e),i(e),a(e),s(e),r(e)}))};n(1021);r.a.render(Object(f.jsx)(a.a.StrictMode,{children:Object(f.jsx)(yt,{})}),document.getElementById("root")),vt()},552:function(e,t,n){},553:function(e,t,n){}},[[1022,1,2]]]);
//# sourceMappingURL=main.408a6956.chunk.js.map